{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks spark Cache vs Persist**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "cache and persist are two methods in Apache Spark that are used to persist (or cache) a DataFrame or RDD in memory or on disk for reusing it across multiple stages of a Spark application. Both methods are used to optimize the performance of iterative algorithms or multiple computations on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Cache\n",
    "    *   It is a more convenient and concise method for caching DataFrames. It is often used for quick caching without worrying about storage level details.\n",
    "    *   It is used to store the data in memory across the working node.\n",
    "    *   we only need to call it on a DataFrame or RDD without specifying storage level details. For **example: df.cache()**\n",
    "    *   \n",
    "*   Persist\n",
    "    *    It offers more flexible as you can choose the storage level explicitly. This flexibility is valuable when you have specific requirements regarding memory usage, serialization, or disk storage.\n",
    "    *    It give choice of storing\n",
    "    *    Mechanism which gives an option to store the data either in-memory or in disc acrosss node\n",
    "    *  The persist method requires you to explicitly provide a storage level, making the syntax slightly more verbose. For example: **df.persist(storageLevel=\"MEMORY_ONLY\").**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both cache and persist can be followed by an unpersist to remove the DataFrame or RDD from the cache. The unpersist method removes the data from memory or disk, depending on the storage level used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example -**\\\n",
    "df1= df.select()\\\n",
    "df2=df1.filter()\\\n",
    "df3=df2.withColumn()\\\n",
    "df4=df2.withColumn()\n",
    "\n",
    "*  without cache or persist\n",
    "    *  df2 is to be calculated again and again which is time and resource consuming.\n",
    "*  with cache \n",
    "    *  we execute the transformation and save the result in the memoery or disk and fetch it when we needed it.\n",
    "    *  Because of this e.g. df3 needed df2 so it will chech the dataframe in memeory and if found it will fecth the result from that. In this the resources and time to calculate the tranformation is saved as now we don't need to re compute df2 it again.\n",
    "    *  but to increase the efficinecy of computation or to decrese exeution time we should't add all the intermediate result to the memory, because these is fixed computation memeory and thus there can be shortage of memory for the other computation, thus we should take care which data we are caching in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Storage level provided by spark in Persist**\n",
    "*   **MEMORY_ONLY**\n",
    "    *   same as cache\n",
    "    *   In this data is stored in deserialized form.\n",
    "*   **MEMORY_ONLY_SER**\n",
    "    *   persisting the RDD in a serialized (binary) form helps to reduce the size of the RDD, thus making space for more RDD to be persisted in the cache memoey, it is space efficient but not time efficient.\n",
    "*   **MEMORY_AND_DISK**\n",
    "    *   Stores partitions on disk which do not fit in memeory.\n",
    "*   **MEMORY_AND _DISK_SER**\n",
    "    *   Same as \"Meomory and disk\" but in serialized form\n",
    "*   **DISK_ONLY**\n",
    "    *   persists data only in disk, which would required network input and output operation thus time consuming. Still better performance than re-computation each time through DAG.\n",
    "*   same as above  but create replication in other nodes. so if any failure of node, still data is accessible through another node.\n",
    "    *   **DISK_ONLY_2**\n",
    "    *   **MEMORY_AND_DISK_2**\n",
    "    *   **MEMORY_ONLY_2**\n",
    "    *  **MEMORY_AND_DISL_SER_2**\n",
    "    *  **MEMORY_ONLY_SER_2**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reason why serialized data takes less memory than de serialized data**\n",
    "\n",
    "When you serialize data, you convert the in-memory representation of objects or data structures into a format that can be easily stored or transmitted, such as a byte stream or a text format like JSON or XML. The serialization process typically removes unnecessary information and organizes the data in a more compact form. This reduction in size is one of the reasons why serialized data can take less space than the original in-memory representation.\n",
    "\n",
    "*   For example, a complex object with various attributes and relationships might be serialized into a simple JSON or binary format where only essential information is included.\n",
    "*   Some serialization formats or libraries include compression techniques to further reduce the size of the serialized data\n",
    "*   During serialization, certain metadata or non-essential information may be stripped away\n",
    "*   Serialization may omit default values for attributes since these can be assumed when deserializing the data. This can result in smaller serialized representations.\n",
    "*   Depending on the serialization method or format used, there might be a degree of lossy compression involved, especially in image or audio data serialization.\n",
    "\n",
    "The degree of space reduction during serialization can vary depending on the specific serialization format, the data being serialized, and the serialization library or tool used.\n",
    "\n",
    "**Note -**\n",
    "*   while serialized data may be smaller for storage or transmission, there is typically some overhead involved in deserialization to reconstruct the original in-memory representation.\n",
    "*   To read the data we need to deserialized the data.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.config(\"spark.driver.host\",\"localhost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Without cache the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\",\",\").load(\"data.csv\")\n",
    "result2 = df2.filter(df2[\"collision_id\"] > 4456624)\n",
    "result2 = df2.filter((df2[\"collision_id\"] < 4456624) & (df2[\"collision_id\"]>4456355))\n",
    "\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = df2.filter((df2[\"collision_id\"] %2==0)).count()\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **with caching the df the execution time reduces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume df is your DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\",\",\").load(\"data.csv\")\n",
    "\n",
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Perform operations on the DataFrame\n",
    "result = df.filter(df[\"collision_id\"] > 4456624)\n",
    "\n",
    "# The DataFrame is now cached in memory, and subsequent actions will be faster\n",
    "# ...\n",
    "result = df.filter((df[\"collision_id\"] < 4456624) & (df[\"collision_id\"]>4456355))\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter((df[\"collision_id\"] %2==0)).count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By doing operatoin of datafarme which is cahched (df) and which is not (df3) ypu can compare the execution time difference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Persist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "df = spark.read.format('csv').options(inferschema=True,header=True,sep=',').load('data.csv')\n",
    "\n",
    "# Cache the DataFrame with different storage levels\n",
    "\n",
    "# 1. MEMORY_ONLY: Store as deserialized Java objects in the JVM. This is the default storage level.\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# 2. MEMORY_ONLY_SER: Store as serialized Java objects (binary format) in the JVM.\n",
    "df.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "\n",
    "# 3. MEMORY_AND_DISK: Store as deserialized Java objects in the JVM, and spill to disk if necessary.\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# 4. MEMORY_AND_DISK_SER: Store as serialized Java objects in the JVM, and spill to disk if necessary.\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "# 5. DISK_ONLY: Store as deserialized Java objects on disk.\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# 6. DISK_ONLY_SER: Store as serialized Java objects (binary format) on disk.\n",
    "df.persist(StorageLevel.DISK_ONLY_SER)\n",
    "\n",
    "# Perform operations on the DataFrame, and Spark will use the cached data\n",
    "result = df.filter(df[\"column\"] > 5).collect()\n",
    "\n",
    "# Unpersist the DataFrame when it's no longer needed\n",
    "df.unpersist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
