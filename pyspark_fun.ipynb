{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions in pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.config('spark.driver.host','localhost').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format('csv').options(header=True, inferschema=True,sep=',').load('data_for_files/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+\n",
      "|collision_id|         crash_date|         crash_time|      on_street_name|number_of_persons_injured|number_of_persons_killed|contributing_factor_vehicle_1|contributing_factor_vehicle_2|  vehicle_type_code1|  vehicle_type_code2|borough|\n",
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+\n",
      "|     4456867|2021-09-02 00:00:00|2024-02-21 19:56:00|MAJOR DEEGAN EXPR...|                        0|                       0|                  Unspecified|                         null|               Sedan|                null|   null|\n",
      "|     4456988|2021-09-11 00:00:00|2024-02-21 15:45:00|     BUSHWICK AVENUE|                        0|                       0|         Following Too Clo...|                  Unspecified|Station Wagon/Spo...|                null|   null|\n",
      "|     4456859|2021-09-07 00:00:00|2024-02-21 22:30:00|          108 AVENUE|                        0|                       0|         Driver Inattentio...|                  Unspecified|Station Wagon/Spo...|Station Wagon/Spo...|   null|\n",
      "|     4456663|2021-06-25 00:00:00|2024-02-21 14:35:00| BEACH CHANNEL DRIVE|                        0|                       0|                  Unspecified|                  Unspecified|       Pick-up Truck|                null| QUEENS|\n",
      "|     4456624|2021-07-08 00:00:00|2024-02-21 18:40:00|AVENUE OF THE AME...|                        0|                       0|         Unsafe Lane Changing|                  Unspecified|               Sedan|               Sedan|   null|\n",
      "|     4456563|2021-07-04 00:00:00|2024-02-21 04:00:00|                null|                        1|                       0|                  Unspecified|                         null|Station Wagon/Spo...|                null| QUEENS|\n",
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a pyspark function that returns a new row for each element in the given array or map. use the default column name col for elements in the array and values for elements in the map. if the array or map is null, that row is eliminated. entire value should be null.\n",
    "\n",
    "array -\\\n",
    "If a column have array datatype and contain more than one value than each element is converted into row.\n",
    "\n",
    "map-\\\n",
    "when a map is passed, it create two new columns one for key and other for value and each element in mapsplit into rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **exploded**\n",
    "    *   if array or map is null than it eliminate that row, the entire value should be null than only it will not include that row.Null if null value is present in array with other element than it include that row.\n",
    "*   **Explode_outer** \n",
    "    *   unlike explode if the array or map has null value than it return null value.\n",
    "*   **Posexplode**\n",
    "    *   when array or map is passed it create position column which contain the index of element in the array or map . It ignore the null elements.\n",
    "*   **posexplode_outer**\n",
    "    *   unlike posexplode, if the array or map is null, explode_outer returns null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - In PySpark, you can use the explode() function on columns that contain array types, not strings directly. If your column contains string values that are formatted as arrays, you can first parse those strings into arrays using functions like split() or regexp_extract(), and then apply explode() on the resulting array column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example for array type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "\n",
    "# Sample array data\n",
    "data = [(1, [10, 20, None,30]), (2, [40, 50]),(3,None)]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"array_data\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|        array_data|\n",
      "+---+------------------+\n",
      "|  1|[10, 20, null, 30]|\n",
      "|  2|          [40, 50]|\n",
      "|  3|              null|\n",
      "+---+------------------+\n",
      "\n",
      "+---+---------+\n",
      "| id|array_col|\n",
      "+---+---------+\n",
      "|  1|       10|\n",
      "|  1|       20|\n",
      "|  1|     null|\n",
      "|  1|       30|\n",
      "|  2|       40|\n",
      "|  2|       50|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.show()\n",
    "df.select(\"id\",explode(\"array_data\").alias(\"array_col\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ablove example you can see (3,None) this row is eliminated as the column on which we apply explode has null value. the row which has null value in explode column that is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explode_outer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|        array_data|\n",
      "+---+------------------+\n",
      "|  1|[10, 20, null, 30]|\n",
      "|  2|          [40, 50]|\n",
      "|  3|              null|\n",
      "+---+------------------+\n",
      "\n",
      "+---+----------+\n",
      "| id|array_data|\n",
      "+---+----------+\n",
      "|  1|        10|\n",
      "|  1|        20|\n",
      "|  1|      null|\n",
      "|  1|        30|\n",
      "|  2|        40|\n",
      "|  2|        50|\n",
      "|  3|      null|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "df.show()\n",
    "df.select(\"id\",explode_outer(\"array_data\").alias(\"array_data\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the row (3,None) is included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**posexplode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|        array_data|\n",
      "+---+------------------+\n",
      "|  1|[10, 20, null, 30]|\n",
      "|  2|          [40, 50]|\n",
      "|  3|              null|\n",
      "+---+------------------+\n",
      "\n",
      "+---+--------+-------+\n",
      "| id|position|element|\n",
      "+---+--------+-------+\n",
      "|  1|       0|     10|\n",
      "|  1|       1|     20|\n",
      "|  1|       2|   null|\n",
      "|  1|       3|     30|\n",
      "|  2|       0|     40|\n",
      "|  2|       1|     50|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Use posexplode to explode the array\n",
    "exploded_df = df.select(\"id\", posexplode(\"array_data\").alias(\"position\", \"element\"))\n",
    "\n",
    "# Show the result\n",
    "exploded_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**posexplode_outer()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|position|array_data|\n",
      "+---+--------+----------+\n",
      "|  1|       0|        10|\n",
      "|  1|       1|        20|\n",
      "|  1|       2|      null|\n",
      "|  1|       3|        30|\n",
      "|  2|       0|        40|\n",
      "|  2|       1|        50|\n",
      "|  3|    null|      null|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode_outer\n",
    "df.select(\"id\",posexplode_outer(\"array_data\").alias(\"position\",\"array_data\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example for map type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In map one column is created for keys and other for values or exploded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with a map column\n",
    "data = [(1, {\"A\": 10, \"B\": 20}), (2, {\"C\": 30, \"D\": 40,\"F\":None}),(3,None)]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"map_data\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\itsrk\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use explode to explode the map\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, explode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use explode to explode the map\n",
    "df.show()\n",
    "df.select(\"id\", explode(\"map_data\").alias(\"key\", \"value\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            map_data|\n",
      "+---+--------------------+\n",
      "|  1|  {A -> 10, B -> 20}|\n",
      "|  2|{C -> 30, D -> 40...|\n",
      "|  3|                null|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   C|   30|\n",
      "|  2|   D|   40|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use of posexplode()\n",
    "from pyspark.sql.functions import explode_outer\n",
    "df.show()\n",
    "df.select(\"id\",explode_outer(\"map_data\").alias(\"key\",\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            map_data|\n",
      "+---+--------------------+\n",
      "|  1|  {A -> 10, B -> 20}|\n",
      "|  2|{C -> 30, D -> 40...|\n",
      "|  3|                null|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+---------+---+-----+\n",
      "| id|posiition|key|value|\n",
      "+---+---------+---+-----+\n",
      "|  1|        0|  A|   10|\n",
      "|  1|        1|  B|   20|\n",
      "|  2|        0|  C|   30|\n",
      "|  2|        1|  D|   40|\n",
      "|  2|        2|  F| null|\n",
      "+---+---------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use of posexplode()\n",
    "from pyspark.sql.functions import posexplode\n",
    "df.show()\n",
    "df.select(\"id\",posexplode(\"map_data\").alias(\"posiition\",\"key\",\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to get the data having id=3 where column have zero value.\n",
    "df=df.select(\"id\",explode_outer(\"map_data\").alias(\"key\",\"value\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **when otherwise case functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **case**\n",
    "   *  its similar to if,elif,else -> when otherwise\n",
    "   *  To evaluate a list of conditions and choose a result path according to the matching condition, when(),otherwise() function in python can be used.\n",
    "   *  this is similar to case or switch statement in other programing language.\n",
    "   *  when no condition is matched than otherwise() result path is exectued.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**syntax**\n",
    "\n",
    ">df.withColumn(\\\n",
    ">    \"new column\",\\\n",
    ">     when(condition , result1)\\\n",
    ">    .when(condition, result2)\\\n",
    ">    .otherwise(result))\n",
    "\n",
    "when(condition,result)\n",
    "*   condition - the condition that is checked.\n",
    "*   result - if condition is true than return the result\n",
    "\n",
    "<br>\n",
    "\n",
    ">df.withColumn(\"new column\",\\\n",
    ">    expr(\"CASE when condition THEN result1 +\\\n",
    ">    \"WHEN condition2 THEN result2 +\\\n",
    ">    \"WHEN condition THEN result3\" +\\\n",
    ">    \"Else result END\"))\n",
    "\n",
    "To combine multile condition: use & (and) , | (or) condition within the when clasue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.withColumn(\"new_col\",when((con1) & (cond2), result1).when((con1)|(con2), result2).otherwise(result3))\n",
    "*   (con1) & (cond2) - of both the consition is true than only rsult1 will be output.\n",
    "*   (con1)|(con2) any or both of the condition should be true than only result 2 will be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   C|   30|\n",
      "|  2|   D|   40|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-------+\n",
      "| id| key|value|new col|\n",
      "+---+----+-----+-------+\n",
      "|  1|   A|   10|    one|\n",
      "|  1|   B|   20|    one|\n",
      "|  2|   C|   30|  other|\n",
      "|  2|   D|   40|  other|\n",
      "|  2|   F| null|  other|\n",
      "|  3|null| null|  three|\n",
      "+---+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.withColumn(\"new col\", when(df.id==1,\"one\").when(df.id==3,\"three\").otherwise(\"other\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---------+\n",
      "| id| key|value|  new col|\n",
      "+---+----+-----+---------+\n",
      "|  1|   A|   10|      low|\n",
      "|  1|   B|   20|      mid|\n",
      "|  2|   C|   30|      mid|\n",
      "|  2|   D|   40|     high|\n",
      "|  2|   F| null|No record|\n",
      "|  3|null| null|No record|\n",
      "+---+----+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull\n",
    "df.withColumn(\"new col\", when(df.value<20,\"low\").when(df.value<=30,\"mid\").when(isnull(\"value\"),\"No record\").otherwise(\"high\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **union and union all**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To combine two dataframe, the scehma of both the dataframe should match (number of column datatype of respective column should be same). \n",
    "\n",
    "* union - in this duplicate value are removed or duplicate value are not included in the dataframe.Remove the duplicate record from resultant dataframe untill spark version 2.0.0. so duplicate can be removed manually by dropDuplicates()\n",
    ">  * df1.union(df2)\n",
    "  * This will merge the record of df1 and df2.\n",
    "* unionall - in this duplicate value are also included.\n",
    ">  * df1.unionall(df2)\n",
    "\n",
    "Note:\n",
    "* if version of spark is greater than 2.0.0 than union and unionAll will give same result and to remove the duplicate we need to use dropDuplicate() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is mismatch on column number than using select() we can select specific columns which are present in other df also and than we can do union betweent two df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   C|   30|\n",
      "|  2|   D|   40|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   C|   30|\n",
      "|  2|   D|   40|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n",
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   C|   30|\n",
      "|  2|   D|   40|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   C|   30|\n",
      "|  2|   D|   40|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(df).show()\n",
    "df.unionAll(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdf=df.union(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **dropDuplicate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   A|   10|\n",
      "|  1|   B|   20|\n",
      "|  2|   D|   40|\n",
      "|  2|   C|   30|\n",
      "|  2|   F| null|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zdf.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicates based on a subset of columns**\\\n",
    "> distinct_subset_df = df.dropDuplicates(subset=[\"id\"])\n",
    "\n",
    "will drop the record if duplicate value present in id column of z dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   A|   10|\n",
      "|  2|   C|   30|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zdf.dropDuplicates(subset=[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pivot and unpivot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivot**\n",
    "*   use to transpose the list of values of a column to column.\n",
    "\n",
    "Pivoting in the context of data transformation refers to the process of rotating or transposing rows into columns, typically to summarize and aggregate data. It involves selecting one column whose unique values will become the new column headers in the output, while another column's values are aggregated across these new columns. Pivoting is useful for reshaping data to make it more readable or suitable for analysis.\\\n",
    "\n",
    "For example, if you have a table with student names, subjects, and scores, you can pivot the table to have student names as rows, subjects as columns, and scores as the intersection of rows and columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unpivot**\n",
    "*   Transposing the columns into list of values to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivo_data= [(\"a\",\"Q1\",6000), (\"x\",\"Q1\",5000), (\"a\",\"Q2\",8000), (\"x\",\"Q2\",7000)]\n",
    "schema=[\"shop\",\"Quater\",\"revenue\"]\n",
    "df_pivo= spark.createDataFrame(data=pivo_data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|shop|Quater|revenue|\n",
      "+----+------+-------+\n",
      "|   a|    Q1|   6000|\n",
      "|   x|    Q1|   5000|\n",
      "|   a|    Q2|   8000|\n",
      "|   x|    Q2|   7000|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|shop|  Q1|  Q2|\n",
      "+----+----+----+\n",
      "|   x|5000|7000|\n",
      "|   a|6000|8000|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new=df_pivo.groupBy(\"shop\").pivot(\"Quater\").sum(\"revenue\")\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+----+\n",
      "|shop|Quater|  Q1|  Q2|\n",
      "+----+------+----+----+\n",
      "|   a|    Q1|6000|null|\n",
      "|   x|    Q1|5000|null|\n",
      "|   a|    Q2|null|8000|\n",
      "|   x|    Q2|null|7000|\n",
      "+----+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivo.groupBy(\"shop\",\"Quater\").pivot(\"Quater\").sum(\"revenue\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, the groupBy clause is commonly used in conjunction with the pivot function when performing pivoting operations. The groupBy clause is used to group the data before applying the pivot operation, allowing you to aggregate or summarize the values within each group.\n",
    "\n",
    "When you pivot data, you typically need to specify an aggregation function (e.g., sum, avg) to determine how values should be combined for each combination of pivot and grouping columns. The groupBy clause provides the context for these aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**un_pivot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|shop|Quater|revenue|\n",
      "+----+------+-------+\n",
      "|   x|    Q1|   5000|\n",
      "|   x|    Q2|   7000|\n",
      "|   a|    Q1|   6000|\n",
      "|   a|    Q2|   8000|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.selectExpr(\"shop\",\"stack(2,'Q1',Q1,'Q2',Q2) as (Quater,revenue)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stack function in PySpark is used to stack or combine multiple columns into two columns: one column for the values from the specified columns, and another column for the associated column names. In your specific case, the stack function is used as follows:\\\n",
    "stack(2, 'Q1', Q1, 'Q2', Q2) as (Quarter, Revenue)\n",
    "\n",
    "*   2: Indicates that you want to stack two columns at a time. In other words, you are combining pairs of columns.\n",
    "\n",
    "*   'Q1', Q1, 'Q2', Q2: These are the columns to be stacked. The pairs are 'Q1', Q1 and 'Q2', Q2. The first element in each pair is a literal string (e.g., 'Q1', 'Q2'), and the second element is the reference to the actual DataFrame column with the corresponding name\n",
    "*   'Q1', Q1 - in 'Q1' is the name or value which we want to see in the output column and Q1 is the column name which is present in the dataframe which we want to stack.\n",
    "\n",
    "\n",
    "\n",
    "So, for each pair, the literal string is used as a label in the output column (e.g., 'Q1' for the first pair, 'Q2' for the second pair), and the values from the referenced columns are combined into a single column.\n",
    "\n",
    "+----+---+---+\\\n",
    "|shop|Q1 &emsp;|Q2|\\\n",
    "+----+---+---+\\\n",
    "| A  &emsp;|10 &emsp;| 20|\\\n",
    "| B  &emsp;|30 &emsp;| 40|\\\n",
    "+----+---+---+\n",
    "\n",
    "\n",
    "Applying the stack(2, 'Q1', Q1, 'Q2', Q2) operation would result in:\n",
    ">df_new.selectExpr(\"shop\",\"stack(2,'Q1',Q1,'Q2',Q2) as (Quater,revenue)\").show()\n",
    "\n",
    "Quarter - The first column contains the literal values 'Q1' or 'Q2' (Quarter).\\\n",
    "revenue - The second column contains the values from the referenced columns ('Q1', 'Q2').\n",
    "\n",
    "'Q1',Q1 - 'Q1' will go to Quater, Q1 will go to revenue\n",
    "\n",
    "e.g.\\\n",
    "Q1 - Q1 , its value is 10 so in result Quarter - Q1 and Revenue - 10\n",
    "\n",
    "\n",
    "+----+-------+-------+\\\n",
    "|shop|Quarter|Revenue|\\\n",
    "+----+-------+-------+\\\n",
    "| A  &emsp;| Q1    &emsp;| 10    &emsp;&emsp;|\\\n",
    "| A  &emsp;| Q1    &emsp;| 10    &emsp;&emsp;|\\\n",
    "| A  &emsp;| Q2    &emsp;| 20    &emsp;&emsp;|\\\n",
    "| B  &emsp;| Q1    &emsp;| 30   &emsp; &emsp;|\\\n",
    "| B  &emsp;| Q2    &emsp;| 40   &emsp;&emsp; |\\\n",
    "+----+-------+-------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UDF user defined funciton**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user defined function is piece of code which perform certain task and can be reused to perform the same task across multiple scenarios.\n",
    "\n",
    "syntax -\n",
    "> def UDF_name(parameters)\\\n",
    "> --code to perform the task\\\n",
    "> --return Return_output\n",
    "\n",
    "UDF - blacl box\n",
    "* UDF is black box for spark engine we cannot apply optimization on the UDF. It is not recomended for databrick or spark development.\n",
    "* Try to minimize the usage of UDF and use built in fucntion.\n",
    "* UDF are created in python or scala but dataframr are in JVM format. so when we call UDF to exectue certain task, it would happen through java API, which require data serialization and deserialization to perform the task. And as UDF black box to spark (as not in JVM), it ca't apply optimization techniques by defualt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 25, None),\n",
    "        (\"Bob\", None, 30),\n",
    "        (\"Charlie\", 35, 40),\n",
    "        (None,52,76)]\n",
    "\n",
    "columns = [\"name\", \"age\", \"score\"]\n",
    "df_udf = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|   name| age|score|\n",
      "+-------+----+-----+\n",
      "|  Alice|  25| null|\n",
      "|    Bob|null|   30|\n",
      "|Charlie|  35|   40|\n",
      "|   null|  52|   76|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define UDF to rename column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "\n",
    "def rename_column(rename_df):\n",
    "    for column in rename_df.columns:\n",
    "        new_column=\"col_\"+column\n",
    "        rename_df =rename_df.withColumnRenamed(column,new_column)\n",
    "\n",
    "        # withColumnRenamed('column_name','Value')\n",
    "\n",
    "    return rename_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+\n",
      "|col_name|col_age|col_score|\n",
      "+--------+-------+---------+\n",
      "|   Alice|     25|     null|\n",
      "|     Bob|   null|       30|\n",
      "| Charlie|     35|       40|\n",
      "|    null|     52|       76|\n",
      "+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_df=rename_column(df_udf)\n",
    "renamed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark function that splits a single column into multiple column based on certain logic\\\n",
    "syntax -\n",
    "> **pyspark.sql.functions.split(str,pattern,limit=-1)**\n",
    "> * str - a string exression to split\n",
    "> * Pattern- a string representing a regular expression\n",
    "> *  limit - optional; an integer that control the number of times pattern is applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with a column containing commas\n",
    "data = [(\"Alice,25,3.5\", \"yt\"),\n",
    "        (\"Bob,30,4.0\", \"rr\"),\n",
    "        (\"Charlie,35,4.5\", \"we\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = [\"data\", \"name\"]\n",
    "split_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|          data|name|\n",
      "+--------------+----+\n",
      "|  Alice,25,3.5|  yt|\n",
      "|    Bob,30,4.0|  rr|\n",
      "|Charlie,35,4.5|  we|\n",
      "+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_split=split_df.withColumn(\"first_col\",split(split_df.data,',',2).getItem(0)).withColumn(\"second_col\",split(split_df.data,',',2).getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---------+----------+\n",
      "|          data|name|first_col|second_col|\n",
      "+--------------+----+---------+----------+\n",
      "|  Alice,25,3.5|  yt|    Alice|    25,3.5|\n",
      "|    Bob,30,4.0|  rr|      Bob|    30,4.0|\n",
      "|Charlie,35,4.5|  we|  Charlie|    35,4.5|\n",
      "+--------------+----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "after_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split(split_df.data,',',2).getItem(0)**\n",
    "\n",
    "*  This expression is used on a string (split_df.data) and splits it into a list of substrings using ',' as the delimiter.\n",
    "*  split_df - is the dataframe, data - is the column present in the dataframe\n",
    "*  The third argument, 2, indicates that the splitting should be done at most 2 times.\n",
    "*  The result is a list of substrings.\n",
    "*  getItem(0) - to get the elemnt at index 0 form the list after split.\n",
    "*  **split_df.data.split(',', 2)[0]** - we can use this format also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write the split query\n",
    "* using select option get the columns instead of withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---------+----------+\n",
      "|          data|name|first_col|second_col|\n",
      "+--------------+----+---------+----------+\n",
      "|  Alice,25,3.5|  yt|    Alice|    25,3.5|\n",
      "|    Bob,30,4.0|  rr|      Bob|    30,4.0|\n",
      "|Charlie,35,4.5|  we|  Charlie|    35,4.5|\n",
      "+--------------+----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_col=split(split_df['data'],',',2)\n",
    "\n",
    "df3=split_df.select(\"data\",\"name\",split_col[0].alias('first_col'),split_col[1].alias('second_col'))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **other use cases**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **we can use different split pattern and use them in one query.**\n",
    "*   Here we are applying split on different columna and using them in one query.\n",
    "> df.withColumn(\"first\",split(df.name,\" \")[0])\\\n",
    ">   .withColumn(\"second\",split(df.name,\" \")[1])\\\n",
    ">   .withColumn(\"start_date\",split(df.date,\"-\")[0])\\\n",
    ">   .withColumn(\"end_date\",split(df.date,\"-\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **split and drop the splitted columns**\n",
    "\n",
    ">df.withColumn(\"first\",split(df.name,\" \")[0])\\\n",
    ">   .withColumn(\"second\",split(df.name,\" \")[1])\\\n",
    ">   .drop(\"df.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=split_df.withColumn(\"first\",split(split_df.data,\",\",2)[0]).withColumn(\"second\",split(split_df.data,\",\",2)[1]).drop(\"df.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------+------+\n",
      "|          data|name|  first|second|\n",
      "+--------------+----+-------+------+\n",
      "|  Alice,25,3.5|  yt|  Alice|25,3.5|\n",
      "|    Bob,30,4.0|  rr|    Bob|30,4.0|\n",
      "|Charlie,35,4.5|  we|Charlie|35,4.5|\n",
      "+--------------+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Arrays_Zip**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark function that return a merged array of struct in which the N-th struct contians all N-th values of input array.\n",
    "* if there are two column having the array datatype and contain array and we want to merge them than we can use array_zip which will merge the two array based on the position of elements into a single elements.\n",
    "* syntax\n",
    ">   **Output = inputDF.withColumn(\"zipped_value\",array_zip(\"array_score1\",\"array_score_2\"))**\n",
    "*  This will merge the two elements into sinlge element in a column.\n",
    "*  we can suppy any number of array_columns and merge them.\n",
    "*  this means we will supply n number of arrays and accoring to posisiton from all array values will be merged, like value of index 0 form all arrays will be merged together and stored in column than value of index 1 will be added together and stored in column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with two arrays\n",
    "data = [(\"Alice\", [25, 30, 35,7], [3.5, 4.0, 4.5]),\n",
    "        (\"Bob\", [28, 32, 37], [3.2, 4.2, 4.7]),\n",
    "        (\"Charlie\", [22, 27, 33], [3.8, 4.1, 4.4])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"name\", \"ages\", \"scores\"]\n",
    "df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+---------------+--------------------------------------------+\n",
      "|name   |ages           |scores         |combined_data                               |\n",
      "+-------+---------------+---------------+--------------------------------------------+\n",
      "|Alice  |[25, 30, 35, 7]|[3.5, 4.0, 4.5]|[{25, 3.5}, {30, 4.0}, {35, 4.5}, {7, null}]|\n",
      "|Bob    |[28, 32, 37]   |[3.2, 4.2, 4.7]|[{28, 3.2}, {32, 4.2}, {37, 4.7}]           |\n",
      "|Charlie|[22, 27, 33]   |[3.8, 4.1, 4.4]|[{22, 3.8}, {27, 4.1}, {33, 4.4}]           |\n",
      "+-------+---------------+---------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_zip, col\n",
    "\n",
    "# Use array_zip to combine two arrays into an array of structs\n",
    "df_result = df.withColumn(\"combined_data\", arrays_zip(col(\"ages\"), col(\"scores\")))\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">+-------+------------+---------------+---------------------------------.+\\\n",
    ">|name   &emsp;&emsp;|ages&emsp;&emsp;&emsp;          |scores&emsp;&emsp;&emsp;&emsp;&emsp;  &emsp;           |combined_data                      \\\n",
    ">+-------+------------+---------------+---------------------------------+\\\n",
    ">|Alice  &emsp; |[25, 30, 35]&emsp;|[25, 30, 35]&emsp;  |[{25, 3.5}, {30, 4.0}, {35, 4.5},{7, null}]|\n",
    "\n",
    "[25, 30, 35],[25, 30, 35] - the elements of two array merged based on position and result is stored in new column combined_data.\n",
    "* If one array contain less number of element than other than null value is added in palce.\n",
    "\n",
    "* If we give only one array than array_zip will split each element of array into seprate element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_brand_exp=df_result.withColumn(\"explode\",explode(\"combined_data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explode()**\\\n",
    "The explode function in PySpark is used to transform a column with arrays or maps into multiple rows, with one row for each element in the array or map. This is particularly useful when you have a column with nested structures, and you want to flatten it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+---------------+--------------------+---------+\n",
      "|   name|           ages|         scores|       combined_data|  explode|\n",
      "+-------+---------------+---------------+--------------------+---------+\n",
      "|  Alice|[25, 30, 35, 7]|[3.5, 4.0, 4.5]|[{25, 3.5}, {30, ...|{25, 3.5}|\n",
      "|  Alice|[25, 30, 35, 7]|[3.5, 4.0, 4.5]|[{25, 3.5}, {30, ...|{30, 4.0}|\n",
      "|  Alice|[25, 30, 35, 7]|[3.5, 4.0, 4.5]|[{25, 3.5}, {30, ...|{35, 4.5}|\n",
      "|  Alice|[25, 30, 35, 7]|[3.5, 4.0, 4.5]|[{25, 3.5}, {30, ...|{7, null}|\n",
      "|    Bob|   [28, 32, 37]|[3.2, 4.2, 4.7]|[{28, 3.2}, {32, ...|{28, 3.2}|\n",
      "|    Bob|   [28, 32, 37]|[3.2, 4.2, 4.7]|[{28, 3.2}, {32, ...|{32, 4.2}|\n",
      "|    Bob|   [28, 32, 37]|[3.2, 4.2, 4.7]|[{28, 3.2}, {32, ...|{37, 4.7}|\n",
      "|Charlie|   [22, 27, 33]|[3.8, 4.1, 4.4]|[{22, 3.8}, {27, ...|{22, 3.8}|\n",
      "|Charlie|   [22, 27, 33]|[3.8, 4.1, 4.4]|[{22, 3.8}, {27, ...|{27, 4.1}|\n",
      "|Charlie|   [22, 27, 33]|[3.8, 4.1, 4.4]|[{22, 3.8}, {27, ...|{33, 4.4}|\n",
      "+-------+---------------+---------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_brand_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see each element of array is flatten out into rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **array_intersect**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark function that return common element across two array without any duplication\n",
    "\n",
    ">**from pyspark.sql.functions import arrays_intersect**\\\n",
    "> **outputDF= inputDF.withColumn(\"intersect\",arrays_intersect(\"col1\",\"col2\"))**\n",
    "\n",
    "*  Intersections is done on the arrays present in col1 and col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+\n",
      "|   name|       ages1|       ages2|\n",
      "+-------+------------+------------+\n",
      "|  Alice|[25, 30, 35]|[30, 35, 40]|\n",
      "|    Bob|[28, 32, 37]|[32, 35, 37]|\n",
      "|Charlie|[28, 21, 33]|[25, 33, 35]|\n",
      "+-------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame with two array columns\n",
    "data = [(\"Alice\", [25, 30, 35], [30, 35, 40]),\n",
    "        (\"Bob\", [28, 32, 37], [32, 35, 37]),\n",
    "        (\"Charlie\", [28, 21, 33], [25, 33, 35])]\n",
    "\n",
    "columns = [\"name\", \"ages1\", \"ages2\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "from pyspark.sql.functions import array_intersect\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+-----------+\n",
      "|   name|       ages1|       ages2|common_ages|\n",
      "+-------+------------+------------+-----------+\n",
      "|  Alice|[25, 30, 35]|[30, 35, 40]|   [30, 35]|\n",
      "|    Bob|[28, 32, 37]|[32, 35, 37]|   [32, 37]|\n",
      "|Charlie|[28, 21, 33]|[25, 33, 35]|       [33]|\n",
      "+-------+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use expr to find the intersection of two array columns\n",
    "df_intersect = df.withColumn(\"common_ages\",array_intersect(\"ages1\", \"ages2\"))\n",
    "df_intersect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **array_except**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark fucntions that return the list of elements which are present in first array but not in second array.\n",
    "> **from pyspark.sql.fucntions import array_except**\\\n",
    "> **output_except= inputDF.withColumns(\"except\",array_except(\"col1\",\"col2\"))**\n",
    "\n",
    "*  Its like col1-col2 which. means element present in col1 except col2. \n",
    "*  common element between col1 and col2 are subtracted form col1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+-----------+\n",
      "|   name|       ages1|       ages2|common_ages|\n",
      "+-------+------------+------------+-----------+\n",
      "|  Alice|[25, 30, 35]|[30, 35, 40]|       [25]|\n",
      "|    Bob|[28, 32, 37]|[32, 35, 37]|       [28]|\n",
      "|Charlie|[28, 21, 33]|[25, 33, 35]|   [28, 21]|\n",
      "+-------+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_except\n",
    "\n",
    "# Use expr to find the intersection of two array columns\n",
    "df_except = df.withColumn(\"common_ages\",array_except(\"ages1\", \"ages2\"))\n",
    "df_except.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **array_sort**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark fucnitons that sorts the elements within the array in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+------------+\n",
      "|   name|       ages1|       ages2|   sort_ages|\n",
      "+-------+------------+------------+------------+\n",
      "|  Alice|[25, 30, 35]|[30, 35, 40]|[25, 30, 35]|\n",
      "|    Bob|[28, 32, 37]|[32, 35, 37]|[28, 32, 37]|\n",
      "|Charlie|[28, 21, 33]|[25, 33, 35]|[21, 28, 33]|\n",
      "+-------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fu\n",
    "\n",
    "df_sort = df.withColumn(\"sort_ages\",fu.array_sort(\"ages1\"))\n",
    "df_sort.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Join the iterable object element**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python function that return a string by joining all the elements of an iterable data such as list, tuple etc seprated by a string seprator.\n",
    "\n",
    "sytax\n",
    "> **string_seprator.join(iterable data)**\n",
    "> *  string_Seprator - seprator string using which iterable elements get joined. it can be , : or can be stirng. It is the seprator based on which you want to join.\n",
    "> *  join - keyword of koin functions\n",
    "> *  iterabel data -data such as list or tuple which can be iterated for joining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'ages1', 'ages2']\n",
      "name-ages1-ages2\n"
     ]
    }
   ],
   "source": [
    "column_list= df.columns\n",
    "print(column_list)\n",
    "\n",
    "join_column= \"-\".join(column_list)\n",
    "print(join_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**column_list= df.columns**\n",
    "* column_list - contain the list of elements.\n",
    "**join_column= \"-\".join(column_list)**\n",
    "* As join will join the iterable object element together but seprated by - in above example.\n",
    "* join_column will return a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **partition by fucniton**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that is used to create partitions based on key and than write them into disk there can be more than one key\n",
    "syntax\n",
    "> **df.write.partitionBy(key).csv(path)**\n",
    "> * csv - file format in which we want to store the partitions on disk.\n",
    "> for each partition a seprate file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.read.format('csv').options(header=True,inferchsmea=True,sep=',').load(\"data_for_files/2011-12-08.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   581214|    23494|VINTAGE DOILY DEL...|      12|2011-12-08 08:32:00|     5.95|   14251.0|United Kingdom|\n",
      "|   581214|    22969|HOMEMADE JAM SCEN...|      60|2011-12-08 08:32:00|     1.45|   14251.0|United Kingdom|\n",
      "|   581214|    22910|PAPER CHAIN KIT V...|      40|2011-12-08 08:32:00|     2.55|   14251.0|United Kingdom|\n",
      "|   581214|    22734|SET OF 6 RIBBONS ...|      12|2011-12-08 08:32:00|     2.89|   14251.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your PySpark code that is causing the error\n",
    "df.write.option(\"header\", True).partitionBy(\"Description\").mode(\"overwrite\").csv(\"tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy(\"Description\")**\n",
    "* partitions is based on dataframes column Description.\n",
    "* there will be seprate file created for each partitions on disk name will be Description =\"value\"\n",
    "* value is the valueof key on which partition happened.\n",
    "\n",
    "**partitionBy(\"Description\",\"InvoiceDate)**\n",
    "* In this first partition is done based on Description column and tha done based on InvoiceDate. thus within Description folder another folder will be created which contain parition of InvoiveDate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To limit the number of record in particular partition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", True).option(\"maxRecordsPerFile\",4200).partitionBy(\"Description\").mode(\"overwrite\").csv(\"/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**opiton(maxRecordsPerFile\",4200)**\n",
    "* this parameter ensure the number of records per partitions file.\n",
    "* parttion happend based on column Description but if partition file contain more than 4200 record than within partition folder file is further partitioned into smaller files based on number of records in file.\n",
    "*  if you set maxRecordsPerFile to 4200, PySpark will ensure that each partition file will contain at most 4200 records. If the number of records in a partition exceeds this limit, PySpark will create additional partition files to accommodate the excess records, ensuring that each partition file stays within the specified limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get number of record in df dataframe.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get number of partition** \n",
    "- we need to change df to RDD to use getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Number of record per partition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionId - 0 , count - 4940\n",
    "* Because there is only one partition, id of partition start from 0.\n",
    "\n",
    "**from pyspark.sql.functions import spark_partition_id**\n",
    "*  The spark_partition_id function in PySpark is used to get the ID of the partition that a row is in. It returns a new column containing the ID of the partition. This function can be useful when you want to inspect or analyze the distribution of data across partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Repartition the dataframe to 5**\n",
    "* becasue of this there will be 5 parition will be created and records are almost equally distributed among different partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5=df.select(df.Description).repartition(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **what if we do partitionBy and than repartition**\n",
    "If you first partition the data based on a particular column and then apply the repartition() function with a different number of partitions, the result will be a shuffle that disregards the previous partitioning scheme and redistributes the data across the new number of partitions.\n",
    "\n",
    "In other words, the previous partitioning based on the specific column value will not be preserved, and the data will be shuffled into the new number of partitions specified by the repartition() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **get number of partitions in a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_5.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get number of record per partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There were 4940 records in df and on parition of df into 5 parts each part get 988 records.\n",
    "* **withColumn(\"partitionId\",spark_partition_id())**\n",
    "  * Each record have another column name partitionId which will tell in which partition the record is present.\n",
    "  * On grouping the df based on partitionId will group the records having same partitionId and than we apply aggregation function count which will give count of record in each group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **count Null value in a dataframes all column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find null occurences of each column in dataframe**\n",
    "* **df.select([count(when(col(c).isNull),c)).alias(c) for c in df.coulumns])**\n",
    "  * when(col(c).isNull) - true when column c have null value.\n",
    "  * count() - will count the record where column c have null value.\n",
    "  * alias(c) - name given to counted column\n",
    "  * select([]) - [] contain list of columns and those will be displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Find top or Bottom N rows per group**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To get the top N record from all partition\n",
    "* we need to partition the df using over() window function\n",
    "* use row_number fucntion which will assign number to each row in each partition.\n",
    "* than we can filter record based on the row number like col<5 will get top 5 record of each col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,row_number,desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowDep=Window.partitionBy(\"Description\").orderBy(col(\"InvoiceNo\").desc())\n",
    "\n",
    "df_row=df.withColumn('row_number',row_number().over(windowDep))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we can use desc or asc according to requirement to get first or bottom rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Top N rows per group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_top= df_row.filter(col(\"row_number\")<=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col(row_number)<=1 - This will give the record who have row_number  less than or equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_top.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Greatest vs least and max vs min**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Greatest\n",
    "  * It is used to get max value across the columns in each row.\n",
    "  * Horizontal scanning of all columns of each row. To get max value in each row.\n",
    "> **from pyspark.sql.functions import greatest**\\\n",
    "> **greatestDF= df.withColumn(\"greatest\",greatest(\"id\",\"value\"))**\n",
    ">  * greatest(\"col1\",\"col2\") - col1, col2 columns in which we want to search to find greatest value out of these columns.\n",
    "* least\n",
    "  * It is used to get min value across the column in each row\n",
    "\n",
    "* max\n",
    "  * this will give max value in a column out of all rows of that column\n",
    "  * vertical scanning of each row of a column specified.\n",
    "* min\n",
    "  * this will give min value in a column out of all rows of that column.\n",
    "\n",
    "> **from pyspark.sql.functions import max,min**\\\n",
    "> **max_value = df.select(max(\"id\"),max(\"value\")).show()**\\\n",
    "> **min_value = df.select(min(\"id\"),min(\"value\")).show()**\n",
    "> *  max(\"id\"),max(\"value\") - id, value are columns in which we want to search the max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1, 10), (2, 5), (3, 8)]\n",
    "columns = [\"id\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import greatest,least,max,min "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get the greatest value across all column in each row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greatestDF= df.withColumn(\"greatest\",greatest(\"id\",\"value\"))\n",
    "greatestDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get the least value across all column in each row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leasttDF= df.withColumn(\"least\",least(\"id\",\"value\"))\n",
    "leasttDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get max, min value in a columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum and minimum values in the 'value' column\n",
    "df.show()\n",
    "max_value = df.select(max(\"id\"),max(\"value\")).show()\n",
    "min_value = df.select(min(\"id\"),min(\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "max_value = df.agg({'id':'max','value':'max'}).show()\n",
    "min_value = df.agg({'id':'min','value':'min'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **input_File_name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is used to identify the input file name for each record that got created in dataframe.\n",
    "* when dataframe is created using different files than with each record there is file name also attached form which file this record came so in case some irregularity comes in data than we can know from which file this data came and than we can correct that data in the file.\n",
    "* otherwise we have to go through all folders files to find the corrupted data.\n",
    "* The input_file_name() function in PySpark doesn't directly get information about which row came from which file. Instead, it provides information about the input file name at the task level. When you use input_file_name() within a transformation, such as map or flatMap, it is executed on each partition independently\n",
    "\n",
    "**Note -**\n",
    "  *   Keep in mind that input_file_name() is only available when reading from Hadoop Distributed File System (HDFS), and it might not work as expected when reading from other file systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Apache Spark, the input_file_name() function provides the name of the file being read by the currently executed task. It is commonly used in PySpark transformations to access information about the input file during data processing. Here's a brief explanation of how input_file_name() works:\n",
    "\n",
    "1. Task Execution in Spark:\n",
    "\n",
    "   *  Spark jobs are divided into tasks, and these tasks are executed on different partitions of the data.\n",
    "   *  Each task processes a portion of the data independently.\n",
    "2. Understanding Input Files:\n",
    "\n",
    "   * When you read data from a directory that contains multiple files, Spark splits the data into partitions, and each task processes a subset of the partitions.\n",
    "   *  Each task is responsible for reading and processing the data from one or more files within its assigned partition.\n",
    "3. Task-Specific File Information:\n",
    "\n",
    "   *  The input_file_name() function is designed to provide information about the input file being processed by the current task.\n",
    "   *  When you use input_file_name() within a Spark transformation, it returns the file name of the current input file for that specific task.\n",
    "4. Usage in Transformations:\n",
    "\n",
    "   *  You can use input_file_name() within Spark transformations, such as map, flatMap, or other functions that operate on the data.\n",
    "   *  By using input_file_name(), you can include the file name information in your transformations, allowing you to customize processing based on the source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =spark.read.format(\"csv\").option(\"recursiveFileLookup\",\"true\").option(\"header\",\"true\").load(\"dbfs:/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **recursiveFileLookup\",\"true\"**\n",
    "  * If folder contain folder within it and in them the files are present than use recursiveFileLookup which will search the sub folders also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name,current_timestamp\n",
    "display(df.withColumn(\"filename\",input_file_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **input_file_name**\n",
    "  * This will fetch the file name from which file particular row data is fetched.\n",
    "  * filename - contain the path of that file or value fetched by input_file_name fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.withColumn(\"filename\",input_file_name()).withColumn(\"date_ingested\",current_timestamp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**current_timestamp** - will give the current time when data is injected into dataframe than at that moment the time will be fetched by the current_timestam and we can find at what time form which file we fetched paticular row in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **window fucntion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [(1, \"Alice\", 100),\n",
    "        (2, \"Bob\", 150),\n",
    "        (3, \"Charlie\", 200),\n",
    "        (4, \"David\", 120),\n",
    "        (5, \"Eva\", 180)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "window_spec = Window.orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use window function we need to import Window class from pyspark.sql.window module.\n",
    "* window_spec = Window.orderBy(\"id\")\n",
    "  * In this example, Window.orderBy(\"id\") creates a Window instance that orders the rows by the \"id\" column. This instance (window_spec)\n",
    "\n",
    "* **partitionBy**\n",
    "  * **window_spec=Window.partitionBy(\"name\").orderBy(\"salary\")**\n",
    "    * window_spec is an instance of the **Window class**, and it represents a window specification. * \n",
    "    * This window specification defines the partitioning and ordering behavior for window functions applied to a DataFrame.\n",
    "\n",
    "    * In this specific case:\n",
    "      * **partitionBy(\"name\")**: It specifies that the rows will be partitioned by the values in the \"name\" column. This means that window functions will operate independently within each partition based on unique values in the \"name\" column.\n",
    "      * **orderBy(\"salary\")**: Within each partition, the rows will be ordered by the \"salary\" column. This defines the order in which the window functions will be applied within each partition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Different functions assosiated with window fucntion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note -** - F is the alias for the class functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+\n",
      "| id|   name|salary|row_number|\n",
      "+---+-------+------+----------+\n",
      "|  1|  Alice|   100|         1|\n",
      "|  2|    Bob|   150|         2|\n",
      "|  3|Charlie|   200|         3|\n",
      "|  4|  David|   120|         4|\n",
      "|  5|    Eva|   180|         5|\n",
      "+---+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rank:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+----+\n",
      "| id|   name|salary|row_number|rank|\n",
      "+---+-------+------+----------+----+\n",
      "|  1|  Alice|   100|         1|   1|\n",
      "|  4|  David|   120|         4|   2|\n",
      "|  2|    Bob|   150|         2|   3|\n",
      "|  5|    Eva|   180|         5|   4|\n",
      "|  3|Charlie|   200|         3|   5|\n",
      "+---+-------+------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"salary\")\n",
    "df = df.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Rank:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(\"salary\")\n",
    "df = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lag:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+----+----------+----------+\n",
      "| id|   name|salary|row_number|rank|dense_rank|lag_salary|\n",
      "+---+-------+------+----------+----+----------+----------+\n",
      "|  1|  Alice|   100|         1|   1|         1|      null|\n",
      "|  2|    Bob|   150|         2|   3|         3|       100|\n",
      "|  3|Charlie|   200|         3|   5|         5|       150|\n",
      "|  4|  David|   120|         4|   2|         2|       200|\n",
      "|  5|    Eva|   180|         5|   4|         4|       120|\n",
      "+---+-------+------+----------+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"id\")\n",
    "df = df.withColumn(\"lag_salary\", F.lag(\"salary\").over(window_spec))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lead:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+----+----------+----------+-----------+\n",
      "| id|   name|salary|row_number|rank|dense_rank|lag_salary|lead_salary|\n",
      "+---+-------+------+----------+----+----------+----------+-----------+\n",
      "|  1|  Alice|   100|         1|   1|         1|      null|        150|\n",
      "|  2|    Bob|   150|         2|   3|         3|       100|        200|\n",
      "|  3|Charlie|   200|         3|   5|         5|       150|        120|\n",
      "|  4|  David|   120|         4|   2|         2|       200|        180|\n",
      "|  5|    Eva|   180|         5|   4|         4|       120|       null|\n",
      "+---+-------+------+----------+----+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"id\")\n",
    "df = df.withColumn(\"lead_salary\", F.lead(\"salary\").over(window_spec))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **convert columns to dictionary (Map)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **create_map**\n",
    ">   **df.select(create_map(cols,col2))**\n",
    ">  * The name of column will become the key and the value become value of the keys.\n",
    "* create_map need 2n paramters where first is key and next is the value.\n",
    "* df.select(create_map(lit(\"shop\"),col2))\n",
    "  * lit() is used to add constant value, col2 is used to give value to the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, \"Alice\", 100),\n",
    "        (2, \"Bob\", 150),\n",
    "        (3, \"Charlie\", 200),\n",
    "        (4, \"David\", 120),\n",
    "        (5, \"Eva\", 180)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"sale\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------------------------+\n",
      "| id|   name|map(name, sale AS shop_sale)|\n",
      "+---+-------+----------------------------+\n",
      "|  1|  Alice|              {Alice -> 100}|\n",
      "|  2|    Bob|                {Bob -> 150}|\n",
      "|  3|Charlie|            {Charlie -> 200}|\n",
      "|  4|  David|              {David -> 120}|\n",
      "|  5|    Eva|                {Eva -> 180}|\n",
      "+---+-------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,create_map,lit, explode\n",
    "df2=df.select(col(\"id\"),col(\"name\"),create_map(col(\"name\"),col(\"sale\").alias(\"shop_sale\")))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------------------------------------+\n",
      "|id |name   |map                                     |\n",
      "+---+-------+----------------------------------------+\n",
      "|1  |Alice  |{shop_name -> Alice, shop_sale -> 100}  |\n",
      "|2  |Bob    |{shop_name -> Bob, shop_sale -> 150}    |\n",
      "|3  |Charlie|{shop_name -> Charlie, shop_sale -> 200}|\n",
      "|4  |David  |{shop_name -> David, shop_sale -> 120}  |\n",
      "|5  |Eva    |{shop_name -> Eva, shop_sale -> 180}    |\n",
      "+---+-------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(col(\"id\"),col(\"name\"),create_map(lit(\"shop_name\"),col(\"name\"),lit(\"shop_sale\"),col(\"sale\")).alias(\"map\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.config('spark.driver.host','localhost').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note -** to get the values from dict into column we can use explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select(explode(\"map\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|                 map|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|{shop_name -> Ali...|\n",
      "|  2|    Bob|{shop_name -> Bob...|\n",
      "|  3|Charlie|{shop_name -> Cha...|\n",
      "|  4|  David|{shop_name -> Dav...|\n",
      "|  5|    Eva|{shop_name -> Eva...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Struct Type vs Map type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Struct Type**\n",
    "  * In this we have to strictly follow the structure. we can add None value if value is not present. \n",
    "  * **StructType([StructField(\"Col1\",dataype,True),StructField(\"Col2\",StructType([StructField(\"Col11\",dataype,True),StructField(\"Col12\",dataype,True)]),True)])**\n",
    "    * While inserting data, this structure must be followed.\n",
    "    * True means null value is allowed.\n",
    "    * e.g. - **\"Ab\"** , {**\"col11\":\"val11\"**,**\"col12\":\"val12\"**}\n",
    "* **Map Type**\n",
    "  * In this there is key value pair and we can add any number of key value pair in that.\n",
    "  * **StructType([StructField(\"Col1\",dataype,True),MapType(datatype1,datatype2,True)])**\n",
    "    * datatype1- datatype of the key\n",
    "    * datatype2 - daataype of the value\n",
    "    * True - null value allowed.\n",
    "    * while adding data we need to mention the key,value pair.\n",
    "    * e.g. - **\"Ab\"** , {**\"col11\":\"val11\"**,**\"col12\":\"val12\"**}\n",
    "\n",
    "Both output might look same but there internal working is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data security: column level data encryption**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cryptography\n",
      "  Downloading cryptography-41.0.7-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting cffi>=1.12\n",
      "  Using cached cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, cffi, cryptography\n",
      "Successfully installed cffi-1.16.0 cryptography-41.0.7 pycparser-2.21\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\projects_py\\spark_test\\.env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# %pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptography.fernet import Fernet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Encryption/Decryption key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = Fernet.generate_key()\n",
    "f=Fernet(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fernet.generate_key()**\n",
    "  * Fernet.generate_key(): This function generates a new random key for use with the Fernet symmetric encryption algorithm. The key is a URL-safe base64-encoded 32-byte key, which can be used both for encryption and decryption.\n",
    "*  **Fernet(key)**\n",
    "   *  This line creates a Fernet object using the key generated in the first step. The Fernet object is then used to perform encryption and decryption operations.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*  key is the actual symmetric encryption key that is generated using Fernet.generate_key().\n",
    "*  f is a Fernet object that uses the key for encryption and decryption operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encrypt sample data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'gAAAAABlcZsWAOREutEI5o7gEjfQcPD2XXPyZCOaZvgK8ZeQsqFlckZCC2zwCyY9znOBDh3R442BJaOibBK2c7wJiK6BWogoWqbMBRoKwMFC7AI3Zqrsj18='\n"
     ]
    }
   ],
   "source": [
    "PIIData = b\"testmail@gamil.com\"\n",
    "TestData= f.encrypt(PIIData)\n",
    "print(TestData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THe calue stored in PIIData is encrypted to TestData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define UDF to Encrypt Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_data(data,KEY):\n",
    "    from cryptography.fernet import Fernet\n",
    "    f=Fernet(KEY)\n",
    "    dataB=bytes(data,'utf-8')\n",
    "    encrypted_data=f.encrypt(dataB)\n",
    "    encrypted_data=str(encrypted_data.decode('ascii'))\n",
    "\n",
    "    return encrypted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The line dataB = bytes(data, 'utf-8') is converting the string data (assuming it's stored in the variable data) into bytes using UTF-8 encoding.\n",
    "\n",
    "*  **data:** This is assumed to be a string variable containing some textual data.\n",
    "*  **bytes(data, 'utf-8'):** This line converts the string data to bytes using the UTF-8 encoding. In Python, strings are Unicode by default, and when you need to represent them as bytes, you specify the encoding (in this case, 'utf-8'). The result is stored in the variable dataB\n",
    "\n",
    "This is useful when working with operations that expect byte-like objects, such as cryptographic functions (like the Fernet encryption you mentioned earlier) or when working with binary data in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define UDF to Decrypt Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_data(encrypted_data,KEY):\n",
    "    from cryptography.fernet import Fernet\n",
    "    f=Fernet(KEY)\n",
    "    decrypted_data=f.decrypt(encrypted_data.encode()).decode()\n",
    "    return decrypted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Register UDF's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,lit,md5\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "encryption = udf(encrypt_data,StringType())\n",
    "decryption = udf(decrypt_data,StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **create dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(1, \"Alice\", 100),\n",
    "        (2, \"Bob\", 150),\n",
    "        (3, \"Charlie\", 200),\n",
    "        (4, \"David\", 120),\n",
    "        (5, \"Eva\", 180)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "encrypteddf = spark.createDataFrame(data, columns)\n",
    "encrypteddf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encrypt the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------------------+\n",
      "| id|   name|salary|      name_encrypted|\n",
      "+---+-------+------+--------------------+\n",
      "|  1|  Alice|   100|gAAAAABlcaN_qv0Cr...|\n",
      "|  2|    Bob|   150|gAAAAABlcaN_-J-9_...|\n",
      "|  3|Charlie|   200|gAAAAABlcaON2dEwK...|\n",
      "|  4|  David|   120|gAAAAABlcaONIEZYq...|\n",
      "|  5|    Eva|   180|gAAAAABlcaOOd1YQe...|\n",
      "+---+-------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encryptDF=encrypteddf.withColumn(\"name_encrypted\",encryption(\"name\",lit(key)))\n",
    "encryptDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decrypt the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------------------+--------------+\n",
      "| id|   name|salary|      name_encrypted|name_decrypted|\n",
      "+---+-------+------+--------------------+--------------+\n",
      "|  1|  Alice|   100|gAAAAABlcaQFb0GwO...|         Alice|\n",
      "|  2|    Bob|   150|gAAAAABlcaQJmYYu1...|           Bob|\n",
      "|  3|Charlie|   200|gAAAAABlcaQX_HHIs...|       Charlie|\n",
      "|  4|  David|   120|gAAAAABlcaQYritOL...|         David|\n",
      "|  5|    Eva|   180|gAAAAABlcaQaDfAjY...|           Eva|\n",
      "+---+-------+------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decryptDF=encryptDF.withColumn(\"name_decrypted\",decryption(\"name_encrypted\",lit(key)))\n",
    "decryptDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Array_repeat** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Array_repeat**\n",
    "    * To create huge amount of data using small data set. \n",
    "    * Create an array containing a value (from a column) reoeated count times.\n",
    "  * syntax\n",
    ">  * **df.withColumn(\"key_col\",array_repeat(col(\"id),5))**\n",
    ">       * This will create a column named key_col which contain array of repeated values as of above example.\n",
    ">       * id - the column whose value we want to repeat.\n",
    ">       * 5 - number of time to repeate the value.    \n",
    "\n",
    "**To create the rows we can apply explode function on the array column which will split the array among different rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(1, \"Alice\", 100),\n",
    "        (2, \"Bob\", 150),\n",
    "        (3, \"Charlie\", 200),\n",
    "        (4, \"David\", 120),\n",
    "        (5, \"Eva\", 180)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "arrayDF = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+\n",
      "| id|   name|salary|key_col|\n",
      "+---+-------+------+-------+\n",
      "|  1|  Alice|   100| [1, 1]|\n",
      "|  2|    Bob|   150| [2, 2]|\n",
      "|  3|Charlie|   200| [3, 3]|\n",
      "|  4|  David|   120| [4, 4]|\n",
      "|  5|    Eva|   180| [5, 5]|\n",
      "+---+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,array_repeat,col\n",
    "repeatDf=arrayDF.withColumn(\"key_col\",array_repeat(col(\"id\"),2))\n",
    "repeatDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+\n",
      "| id| name|salary|key_col|\n",
      "+---+-----+------+-------+\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "|  1|Alice|   100|      1|\n",
      "+---+-----+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.withColumn(\"key_col\",explode(array_repeat(col(\"id\"),100)))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As id is of different value thus we need to do some changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+----+-----+\n",
      "| id| name|salary|key_col|rowN|newID|\n",
      "+---+-----+------+-------+----+-----+\n",
      "|  1|Alice|   100|      1|   1|    2|\n",
      "|  1|Alice|   100|      1|   2|    3|\n",
      "|  1|Alice|   100|      1|   3|    4|\n",
      "|  1|Alice|   100|      1|   4|    5|\n",
      "|  1|Alice|   100|      1|   5|    6|\n",
      "|  1|Alice|   100|      1|   6|    7|\n",
      "|  1|Alice|   100|      1|   7|    8|\n",
      "|  1|Alice|   100|      1|   8|    9|\n",
      "|  1|Alice|   100|      1|   9|   10|\n",
      "|  1|Alice|   100|      1|  10|   11|\n",
      "|  1|Alice|   100|      1|  11|   12|\n",
      "|  1|Alice|   100|      1|  12|   13|\n",
      "|  1|Alice|   100|      1|  13|   14|\n",
      "|  1|Alice|   100|      1|  14|   15|\n",
      "|  1|Alice|   100|      1|  15|   16|\n",
      "|  1|Alice|   100|      1|  16|   17|\n",
      "|  1|Alice|   100|      1|  17|   18|\n",
      "|  1|Alice|   100|      1|  18|   19|\n",
      "|  1|Alice|   100|      1|  19|   20|\n",
      "|  1|Alice|   100|      1|  20|   21|\n",
      "+---+-----+------+-------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lit,row_number\n",
    "\n",
    "win= Window.orderBy(lit('A'))\n",
    "df2=df1.withColumn(\"rowN\",row_number().over(win))\n",
    "df3=df2.withColumn(\"newID\",col(\"id\")+col(\"rowN\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DRoping the unwanted columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  2|Alice|   100|\n",
      "|  3|Alice|   100|\n",
      "|  4|Alice|   100|\n",
      "|  5|Alice|   100|\n",
      "|  6|Alice|   100|\n",
      "|  7|Alice|   100|\n",
      "|  8|Alice|   100|\n",
      "|  9|Alice|   100|\n",
      "| 10|Alice|   100|\n",
      "| 11|Alice|   100|\n",
      "| 12|Alice|   100|\n",
      "| 13|Alice|   100|\n",
      "| 14|Alice|   100|\n",
      "| 15|Alice|   100|\n",
      "| 16|Alice|   100|\n",
      "| 17|Alice|   100|\n",
      "| 18|Alice|   100|\n",
      "| 19|Alice|   100|\n",
      "| 20|Alice|   100|\n",
      "| 21|Alice|   100|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputDF= df3.drop(\"Id\",\"key_col\",\"rowN\").select(col(\"newID\").alias(\"id\"),col(\"name\"),col(\"salary\"))\n",
    "\n",
    "outputDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **subtract vs exceptAll**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both are the pyspark transformation used in spark development \n",
    "* These transformatin create new dataframe by containing the rows from one dataframe which are not present in second dataframe\n",
    "* subtract does not preserve duplicate while except all preserves duplicate while creating new dataframe.\n",
    "* exceptAll is like canceling the records, it will not remove the duplicate values if they match with rows present in second dataframe. As in subtrat all the record are removed.\n",
    "\n",
    "syntax\n",
    "  * df1.subtract(df2)\n",
    "  * df1.executeAll(df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
