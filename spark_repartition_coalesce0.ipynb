{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks spark: Repartitoin vs coalesce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition playes important role in performance improvement, error handling  and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need of partition strategy\n",
    "* Adopting best partition strategy is designing best performance in spark application.\n",
    "* It is important to choose right number of partitions and size of each parition.\n",
    "  * e.g.\n",
    "    * we did 10 paritions and there are 16 cores so suring processing each core will process each parititon and as there are only 10 parition (parittion cannot be further breakdown so to spilt it among the cores) then these paritions will be processed by 10 core nut in this the remaining 6 core will remain ideal or unused which is wastage of resources. Thus we try to parition the data into number of cores present.\n",
    "* Evenly distributed parition improves the performace, unevenly distributed hits the performance.\n",
    "  * with even distribution reources are effectively used like all cores are used. \n",
    "  * try to make parition equal to cores or number should be multiple of number of cores.\n",
    "* Size of the partition should be choosen carefully.\n",
    "  * lets say only one partition is created with size of 500 MB in a worker node with 16 core. One partition can't be shared among cores. so one core would be processing 500MB data where 15 cores kept idle. thus \n",
    "  * or lets say one parittion is of size 1GB and other 100MB thus partition of 100MB will be processed quickly and for 1GB it will take time. thus the core which processes 100MB file will remain in idle state for large amount of time which is like wastage of the services thus we should do partition in proper size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default partitions for RDD and dataframe**\n",
    "* The parameter **sc.defaultParallelism** determines the number of partition when creating data within spark. default value is 8 so it create 8 parition by default.\n",
    "  * sc.defaultParallelsim - when creating data within spark.\n",
    "* when reading data from external system, paritions are created based on parameter **spark.sql.files.maxPartitionBytes** which is by default 128 MB.\n",
    "  * sc.files.maxPartitionByte - when we are reading file from external source. The spark will paritition the file basesd on size of 128 MB each partition.\n",
    "  * Applicable only when file is splitable, if it is zip file than there will be only one partition.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.config(\"spark.driver.host\",'localhost').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Partitions by changing the parameters spark.sql.files.maxPartitionBytes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the maxpartitionBytes parameter which changes in no of partitions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'134217728b'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.mazPartitionBytes\",100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conf= spark.read.format(\"csv\").option(\"inferschema\",\"true\").option(\"header\",\"True\").option(\"sep\",',').load(\"data.csv\")\n",
    "df_conf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets say data.csv has size=477486 than with  spark.conf.set(\"spark.sql.files.mazPartitionBytes\",100000) setting there will be 5 parition will be craeted each of 100Kb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To reset the parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.unset(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'134217728b'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example - how spark.sql.files.maxPartitionBytes will effect the number of partitions**\n",
    ">**dbutils.fs.ls('dbfs:/FileStore/tables/')**\n",
    "\n",
    "suppose the tables contain two **files 2011-12-05.csv** of size=477486  and **2011-12-08.csv** of size=437903 total size of both the files is 9,15,389\n",
    "\n",
    ">**spark.conf.set(\"spark.sql.files.maxPartitionBytes\",200000)**\n",
    "\n",
    "we set the configuration parameter spark.sql.files.maxPartitionBytes to a value of 200,000\n",
    "\n",
    ">**df=spark.read.format(\"csv\").option(\"inferschema\",\"true\").option(\"header\",\"true\").option(\"sep\",\",\").csv('dbfs:/FileStore/tables/')**\n",
    "\n",
    "\n",
    "This will read all the files present at the path defined and will craete a dataframe.\\\n",
    "**df.rdd.getNumPartitions()**\\\n",
    "-- this will give o/p = 6\n",
    "\n",
    "That measn there will be six partition of the df where n-1 partition of size 200000 and remaining 1 partition will have remaining portion of file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Repartition and coalesce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note -**\n",
    "*   Both repartition and coalesce are transformations, and they do not trigger immediate execution. They are evaluated lazily, and the actual data movement occurs during subsequent actions, such as writing to disk or performing certain operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Repartition**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function reparition is used to increase and decrease the partition in spark.\n",
    "* It is a mehtod in spark which is used to perform a full shuffle on the data present and createds partitoins based on user's input. The resulting data is hash paritioned and the data is equally distributed among the partitions.\n",
    "* Repartition always shuffle the data and build new partition from the scratch.\n",
    "  * shufflying is costlier process.\n",
    "  * full shuffle is done in this before repartition.\n",
    "  * reparition always involve a shuffle.\n",
    "* repartition result in almost equal sized partition. After the shufflying repartition is done in which parittion of resultant file is done in equal size. \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what happens when we want to decrease the number of partitons.**\n",
    "\n",
    "*   When you use repartition to decrease the number of partitions, it performs a full shuffle of the data.\n",
    "*   A full shuffle means that the data is redistributed across the specified number of partitions, involving data movement across the network.\n",
    "*   This operation is more expensive in terms of computational resources compared to coalesce.\n",
    "repartition is often used when you need to adjust the number of partitions and achieve a more balanced distribution of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Its effect**\\\n",
    "in PySpark, the repartition operation not only redistributes data across partitions in memory but also affects the physical storage layout on disk when the data is persisted. When you repartition a DataFrame and then write it to a storage system (e.g., HDFS, S3, local file system), the data will be stored in the specified number of files or directories.\n",
    "\n",
    "* **Repartitioning in Memory:**\n",
    "    * When you call repartition on a DataFrame, it shuffles the data across the specified number of partitions in memory. This is done to create a more balanced distribution of data among the partitions, which can improve the efficiency of subsequent operations.\n",
    "\n",
    "*  **Writing to Disk:**\n",
    "    * When you write a DataFrame to an external storage system using methods like write.parquet(), write.csv(), or others, the data is physically stored on disk. The number of files or directories created on disk corresponds to the number of partitions in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|   John| 25|\n",
      "|   Jane| 30|\n",
      "|    Bob| 22|\n",
      "|  Alice| 28|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"John\", 25), (\"Jane\", 30), (\"Bob\", 22), (\"Alice\", 28), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get number of partitions in the original Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original partitions: 12\n"
     ]
    }
   ],
   "source": [
    "# Get the number of partitions in the original DataFrame\n",
    "original_partitions = df.rdd.getNumPartitions()\n",
    "print(f\"Number of original partitions: {original_partitions}\")\n",
    "\n",
    "# o/p - Number of original partitions: 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Repartition the DataFrame into a specified number of partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 3\n",
    "df_repartitioned = df.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df_repartitioned = df.repartition(num_partitions)**\n",
    "* This will reparittion the dataframe into 2 partitions.\n",
    "* we can write these parition to a specific location.\n",
    "* e.g.  \n",
    "  * df.repartition(2).write.mode('overwrite').parquet('/new_test')\n",
    "    * This will parition the df dataframe in two partitoins and those paritions are saves in the location mentioned.\n",
    "    * parquet('/new_test')\n",
    "      * At location /new_test the dataframe is stored in parquet format.\n",
    "        * /new_test is  - dbfs:/news_test\n",
    "      * At this location two files will be created as we have doen two partitions of dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Name: string, Age: bigint]\n"
     ]
    }
   ],
   "source": [
    "print(df_repartitioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the number of partitions in the repartitioned DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new partitions: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "print(f\"Number of new partitions: {new_partitions}\")\n",
    "\n",
    "# o/p -Number of new partitions: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To check the data present in the paritions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='dbfs:/new_test/part-00000.parquet'\n",
    "\n",
    "# part-00000.parquet - name of file created after partition.\n",
    "\n",
    "\n",
    "# Read the partitioned parquet file into a DataFrame\n",
    "df_parquet = spark.read.parquet(path, header=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='dbfs:/new_test/part-00001.parquet'\n",
    "\n",
    "# part-00000.parquet - name of file created after partition.\n",
    "\n",
    "\n",
    "# Read the partitioned parquet file into a DataFrame\n",
    "df_parquet = spark.read.parquet(path, header=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='dbfs:/new_test/part-00003.parquet'\n",
    "\n",
    "# part-00000.parquet - name of file created after partition.\n",
    "\n",
    "\n",
    "# Read the partitioned parquet file into a DataFrame\n",
    "df_parquet = spark.read.parquet(path, header=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coalesce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coalesce function only reduce the number of the partitions.\n",
    "* Coalsce does't require a full shuffle.\n",
    "* Coalesce combine few partitions or shuffle data from few partitions thus avoiding funll shuffle.\n",
    "* Due to partition merge, it produce uneven size of partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When you use coalesce, it tries to minimize data movement and achieve the target number of partitions by merging adjacent partitions without a full shuffle. It is a more efficient operation compared to repartition when decreasing the number of partitions. However, it doesn't guarantee an even distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To reduce the partitions, using coalesce we created two partitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. when we want to directly partition the dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supppose df had 4 partitions. using coalesce we will be decreasing the number of partitions.\n",
    ">df_col = df.coalesce(2)\n",
    "*   Now df data frame will be repartitioned to 2 partition.\n",
    "*   coalesce operation is indeed a transformation, and when you call df.coalesce(2), it returns a new DataFrame.\n",
    "*   The variable df_col now holds a reference to a new DataFrame that has been coalesced to 2 partitions. The actual computation and data movement associated with the coalesce transformation will occur when you perform an action on this DataFrame.\n",
    "\n",
    "\n",
    "when you use transformations like coalesce in PySpark, it creates a new DataFrame with the specified transformations applied, and it retains a logical execution plan. This logical plan contains the series of transformations to be applied, but the actual execution (evaluation of the plan and data movement) is deferred until an action is called.\n",
    "\n",
    "In the case of df_col = df.coalesce(2), the variable df_col is a reference to a new DataFrame with the coalesce transformation specified. The execution plan, including the coalesce operation, is part of the DataFrame's logical plan.\n",
    "\n",
    "When you perform an action, such as calling show(), collect(), or writing to a file, PySpark triggers the physical execution of the logical plan. At this point, the coalesce operation is executed, and the data is moved accordingly.\n",
    "\n",
    "\n",
    "\n",
    "**To check the number of partition after coalesce.**\\\n",
    "new_partitions_coalesce = df_col.rdd.getNumPartitions()\n",
    "print(f\"Number of new partitions: {new_partitions_coalesce}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. when a folder contain number of partioned files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   We will first read all the files, create dataframe and then do partition using coalesce()\n",
    "*   we can write the partitioned dataframe to disk in particular format.\n",
    "*   The number of files craeted will be equal to number of partitions we did.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Coalesce the DataFrame to 2 partitions (minimizing data movement)\n",
    "df=spark.read.parquet('/new_test')\n",
    "df_col = df.coalesce(2)\n",
    "df3=df_col.write.mode('overwrite').parquet('/new_col')\n",
    "\n",
    "\n",
    "new_partitions_coalesce = df_col.rdd.getNumPartitions()\n",
    "print(f\"Number of new partitions: {new_partitions_coalesce}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **spark.read.parquet('/new_test')**\n",
    "   *  /new_test - location from where we will read the files\n",
    "*  **df_col = df.coalesce(2)**\n",
    "   *  It creates a new DataFrame with the specified transformations applied, and it retains a logical execution plan. This logical plan contains the series of transformations to be applied, but the actual execution (evaluation of the plan and data movement) is deferred until an action is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **repartitions vs coalesce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  reparition \n",
    "    *  repartition: When you use repartition, a full shuffle occurs, and the data is indeed moved between partitions across the network. The goal is to create a new set of partitions according to the specified number. During this process, the data is reorganized, and there's substantial network communication.\n",
    "    *  It involves a full shuffle of the data, making it a relatively expensive operation, especially for large datasets.\n",
    "\n",
    "    *  Use repartition when you want to increase or decrease the number of partitions and aim to achieve a more balanced distribution of data.\n",
    "\n",
    "    *  It can be more computationally expensive compared to coalesce, especially when increasing the number of partitions\n",
    "    *  If you need to increase the number of partitions or achieve a more balanced distribution, use repartition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  coalesce:\n",
    "\n",
    "    *  coalesce is a transformation that reduces the number of partitions without a full shuffle of data.\n",
    "\n",
    "    *  It works by merging adjacent partitions, trying to minimize data movement.\n",
    "\n",
    "    *  coalesce is more efficient than repartition when decreasing the number of partitions.\n",
    "\n",
    "    *  Use coalesce when you want to decrease the number of partitions, and achieving a perfectly balanced distribution is not as critical.\n",
    "\n",
    "    *  It is generally faster than repartition, especially when decreasing the number of partitions\n",
    "    *  If you need to decrease the number of partitions and achieving a perfect balance is not critical, use coalesce for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both repartition and coalesce are transformations and do not trigger immediate execution. The actual data movement occurs during subsequent actions, such as writing to disk or performing certain operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
