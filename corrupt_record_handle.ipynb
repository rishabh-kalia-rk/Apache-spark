{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =SparkSession.builder.config('spark.driver.host','localhost').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample data for this program** -\\\n",
    "file name - corrupt_data.csv\n",
    "\n",
    "month,emp_count,production_unit,expense\\\n",
    "jan,244,657,30\\\n",
    "feb,54,876,32\\\n",
    "mar,37,673,82\\\n",
    "apr,72,721,72,test_msg\\\n",
    "may,29,weare,63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **create sample dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format('csv').options(header=True,sep=',',inferschema=True).load(\"corrupt_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+\n",
      "|month|emp_count|production_unit|expense|\n",
      "+-----+---------+---------------+-------+\n",
      "|  jan|      244|            657|     30|\n",
      "|  feb|       54|            876|     32|\n",
      "|  mar|       37|            673|     82|\n",
      "|  apr|       72|            721|     72|\n",
      "|  may|       29|          weare|     63|\n",
      "+-----+---------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "# this is not showing the 5the value added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"month\",StringType(),True),\n",
    "                    StructField(\"emp_count\",IntegerType(),True),\n",
    "                    StructField(\"production_unit\",IntegerType(),True),\n",
    "                    StructField(\"expense\",IntegerType(),True),\n",
    "                    StructField(\"_corrupt_record\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **permissive mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for your CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"emp_count\", IntegerType(), True),\n",
    "    StructField(\"production_unit\", IntegerType(), True),\n",
    "    StructField(\"expense\", IntegerType(), True),\n",
    "    StructField(\"_corrupt_record\",StringType(),True)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+----------------------+\n",
      "|month|emp_count|production_unit|expense|_corrupt_record       |\n",
      "+-----+---------+---------------+-------+----------------------+\n",
      "|jan  |244      |657            |30     |null                  |\n",
      "|feb  |54       |876            |32     |null                  |\n",
      "|mar  |37       |673            |82     |null                  |\n",
      "|apr  |72       |721            |72     |apr,72,721,72,test_msg|\n",
      "|may  |29       |null           |63     |may,29,weare,63       |\n",
      "+-----+---------+---------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file with the specified options\n",
    "df2 = spark.read.format('csv') \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", ',') \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"corrupt_data.csv\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with table if some corrupt datatype value is present than that is shown with null value and the corrupt record is mentioned in _corrupt_recoed column.\n",
    "\n",
    "As in record of month apr - there are 5 values but as we have defined scehma of 4 column only thus this record is corrupt.\n",
    "\n",
    "similar for month of may in productoin_unit string value present but according to datatype it should be integer type thus this is also a corrupt record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **dropMallformed Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for your CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"emp_count\", IntegerType(), True),\n",
    "    StructField(\"production_unit\", IntegerType(), True),\n",
    "    StructField(\"expense\", IntegerType(), True),\n",
    "    StructField(\"_corrupt_record\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=spark.read.format('csv').option(\"mode\",\"DROPMALFORMED\").option(\"header\",True).option(\"sep\",\",\").schema(schema).load(\"corrupt_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+---------------+\n",
      "|month|emp_count|production_unit|expense|_corrupt_record|\n",
      "+-----+---------+---------------+-------+---------------+\n",
      "|  jan|      244|            657|     30|           null|\n",
      "|  feb|       54|            876|     32|           null|\n",
      "|  mar|       37|            673|     82|           null|\n",
      "+-----+---------+---------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the record of apr, may are not present because those were corrupt record so DROPMALFORMED will drop the corrupt record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FailFast Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for your CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"emp_count\", IntegerType(), True),\n",
    "    StructField(\"production_unit\", IntegerType(), True),\n",
    "    StructField(\"expense\", IntegerType(), True),\n",
    "    StructField(\"_corrupt_record\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=spark.read.format('csv').option(\"mode\",\"FAILFAST\").option(\"header\",True).option(\"sep\",\",\").schema(schema).load(\"corrupt_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df4.show() - will give error if any corrupt record is encountered, even data frame will not be created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
