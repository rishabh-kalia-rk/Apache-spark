{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks spark: Structured streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Streaming Terminologies**\n",
    "* **Readstream:** to read the streaming data, this is the entry point API or function using this we can get the data from outside source. Source can be file system, Database, or streaming service called kafka.\n",
    "* **writestream:** to write the streaming data, once we get the data from source apply transformation on it we can then need to store it somewhere.\n",
    "* **Checkpoint:** It is used to load incremental data loading if we want incremental loading than we need to keep track of precessed data and info about previous state also.\n",
    "  * checkpoint retain the previous value or in previous run and next time when we run the prcoess the spark will refer the checkpoint value or location so understand the processed and unprocessed data. It will pick only unprocessed incremental data only.\n",
    "  *  checkpoint plays key role in fault tolerant and incremental stream processing pipeline. It maintains intermediate state on HDFS compatible file system to recover from failures. And we can then start from this point \n",
    "* **Trigger:** data continuously flows into a streaming system. The special event trigger initiates the streaming. Default, fixed interval, one- time \n",
    "  * default  - in this the incoming data is segrigated as microbatch and one micro batch is processed at a time. The next prosseing is done only when previous batch processing is finished.\n",
    "  * Fixed interval - in this the we mention the fixed interval time and after that interval the incoming data is processed.\n",
    "  * One time - In this all the data is processed at one instant.\n",
    "* **Output mode:** append, complete, update.\n",
    "  * Append - only the new or incremental data is written to the output sink. It is suitable when you are accumulating data over time and only want to append new results.\n",
    "  * output - the entire result table is written to the output sink after each micro-batch. It is suitable when you want the complete result at each point in time.\n",
    "  * update - only the rows that have changed since the last micro-batch are written to the output sink. It is suitable for scenarios where you want to track changes in the result.\n",
    "\n",
    "> query = df.writeStream.outputMode(\"mode\").format(\"console\").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standard rchitecture in streaming data processsing**\n",
    "*  Ingest\n",
    "*  Process\n",
    "*  Store\n",
    "*  Report\n",
    "\n",
    "Databrick can fetch the data directly from the source or using mediators like kafka or Azure events hubs.\n",
    "\n",
    "If the raw data is need for tracking or for debugging processing than we can store it in first in storage location. This location can also be used for checkpoint purpose also.\n",
    "\n",
    "After the data is inject to databricks, after doing the bussiness logic on it we can then store the processed data in the Azure data warehouse.\n",
    "\n",
    "We can then directly send data for reporting or can use azure services in between to create the schema declared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
