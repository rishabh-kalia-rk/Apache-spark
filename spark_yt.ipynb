{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findspark library to initialize and configure Spark within a Python environment. This is typically done when you want to use Apache Spark in a local mode (standalone) on your machine or in a development environment.\n",
    "\n",
    "**import findspark:** This imports the findspark Python library, which is a lightweight package that makes it easier to locate Spark within the system and set the necessary environment variables.\n",
    "\n",
    "**findspark.init():** This function call initializes the Spark environment. It tries to locate Spark (assuming it's installed on your machine) and sets the required environment variables, such as SPARK_HOME and PYSPARK_PYTHON, to make Spark accessible from your Python environment.\n",
    "\n",
    "**Note:**\n",
    "Keep in mind that for this to work, you should have Spark installed on your machine, and the SPARK_HOME environment variable should be set correctly. Additionally, the findspark library is not required if you are working in a Spark cluster environment (such as using Databricks) where Spark is already configured. The primary use of findspark is for local development setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.host\", \"localhost\").getOrCreate()\n",
    "\n",
    "#to build session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('rdddf').master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration option spark.driver.host is used to set the host or IP address that the Spark driver program will bind to on the local machine. The driver program is the main control program of a Spark application, responsible for coordinating the execution of tasks on worker nodes.\n",
    "\n",
    "spark.driver.host: This is a Spark configuration property that determines the host or IP address that the Spark driver should bind to. By setting it to \"localhost,\" you are instructing Spark to bind the driver program to the local machine.\n",
    "\n",
    "\"localhost\": This is the value assigned to the spark.driver.host configuration property. It means that the Spark driver program will run on the same machine where the Spark application is launched. This is typical in local development environments or when running Spark on a single machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.read.format('csv').options(inferSchema=True, header= True, sep=',').load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use many option or use options to set all properties together.\\\n",
    "e.g.\n",
    "\n",
    "df= spark.read.format('csv').option(\"inferSchema\",True).option(\"header\", True).option(\"sep\",',').load(\"data.csv\")\\\n",
    "or\\\n",
    "df= spark.read.format('csv').options(inferSchema=True, header= True, sep=',').load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+\n",
      "|collision_id|         crash_date|         crash_time|      on_street_name|number_of_persons_injured|number_of_persons_killed|contributing_factor_vehicle_1|contributing_factor_vehicle_2|  vehicle_type_code1|  vehicle_type_code2|borough|\n",
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+\n",
      "|     4456867|2021-09-02 00:00:00|2023-11-30 19:56:00|MAJOR DEEGAN EXPR...|                        0|                       0|                  Unspecified|                         null|               Sedan|                null|   null|\n",
      "|     4456988|2021-09-11 00:00:00|2023-11-30 15:45:00|     BUSHWICK AVENUE|                        0|                       0|         Following Too Clo...|                  Unspecified|Station Wagon/Spo...|                null|   null|\n",
      "|     4456859|2021-09-07 00:00:00|2023-11-30 22:30:00|          108 AVENUE|                        0|                       0|         Driver Inattentio...|                  Unspecified|Station Wagon/Spo...|Station Wagon/Spo...|   null|\n",
      "|     4456663|2021-06-25 00:00:00|2023-11-30 14:35:00| BEACH CHANNEL DRIVE|                        0|                       0|                  Unspecified|                  Unspecified|       Pick-up Truck|                null| QUEENS|\n",
      "|     4456624|2021-07-08 00:00:00|2023-11-30 18:40:00|AVENUE OF THE AME...|                        0|                       0|         Unsafe Lane Changing|                  Unspecified|               Sedan|               Sedan|   null|\n",
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849\n"
     ]
    }
   ],
   "source": [
    "display(df.show(5))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If want to define schema manually**\n",
    "\n",
    "Example - \n",
    "\n",
    "--to send the schema first scehma is defined.\\\n",
    "-- when we manually deifine the scehma than we don't need to use inferScehma\n",
    "\n",
    ">from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    ">\n",
    ">\n",
    ">scehma_define= StructType(\\\n",
    ">StructField(\"id\",IntegerType(),True),\\\n",
    ">StructField(\"name\",\"StringType(),True),\\\n",
    ">StructField(\"roll_number\",IntegerType(),True)\\\n",
    ">)\n",
    "\n",
    "In structField('column_name','data_type','True')\\\n",
    "True: Column allows null values.\\\n",
    "False: Column does not allow null values.\n",
    "\n",
    "\n",
    "**df = spark.read.format('csv').option(header=True, sep=',').schema(scehma_define)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **To check the schema of the dataframe use data_frame.printSchema()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- collision_id: integer (nullable = true)\n",
      " |-- crash_date: timestamp (nullable = true)\n",
      " |-- crash_time: timestamp (nullable = true)\n",
      " |-- on_street_name: string (nullable = true)\n",
      " |-- number_of_persons_injured: integer (nullable = true)\n",
      " |-- number_of_persons_killed: integer (nullable = true)\n",
      " |-- contributing_factor_vehicle_1: string (nullable = true)\n",
      " |-- contributing_factor_vehicle_2: string (nullable = true)\n",
      " |-- vehicle_type_code1: string (nullable = true)\n",
      " |-- vehicle_type_code2: string (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To read multiple csv files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are reading multiple files into a DataFrame in PySpark, it is generally advisable that the files have the same schema, including the same number of columns and corresponding data types. This ensures consistency in your DataFrame and prevents issues that may arise from inconsistent or mismatched schemas.\n",
    "\n",
    "Example -\\\n",
    "**df=spark.read.format('csv').options(header=True,sep=',').load([\"path1/file1.csv\",\"path2/file2.csv\"])**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read all files within a  particular folder**\n",
    "**df= spark.read.format('csv').options(inferSchema=True,header=True, sep=',').load(\"path/folder\")**\\\n",
    "Thus all the file within the folder spark will read them. But bydefault it it won't automatically traverse nested directories.\n",
    "\n",
    "**df = spark.read.format('csv').options(inferSchema=True, header=True, sep=',',recursiveFileLookup =True).load(\"path/folder\")**\\\n",
    "By adding recursiveFileLookup=True, you are telling PySpark to recursively search for files in subdirectories of the specified path.\n",
    "\n",
    "Note:\\\n",
    "Keep in mind that while recursiveFileLookup allows for recursive directory discovery, it assumes that the data in those nested folders has the same schema as the top-level folder. If the nested folders have different schemas, you might need to handle them separately or implement a more dynamic schema resolution strategy based on your specific use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pyspark filter condition**\n",
    "\n",
    "filter()\n",
    "1. **single and multiple conditoin** - (df.col('id')>4 & df.col('col2')>32)\n",
    "2. **starts with** - df.column.startswith('char')\n",
    "3. **endwith** - df.columns.endswith('char')\n",
    "4. **contains** - df.column.contains('string')\n",
    "   1. contains - to check if string present in column value or not.\n",
    "5. **like**- df.column.like('%a')\n",
    "6. **null** value - df.column.isNull('cal') - to get record whose column 'cal' value is null.\n",
    "7. **not null** - df.column.NotNull('cal') - to get record in which 'cal' don't have null value.\n",
    "8. **isin** - df.column.isin(char1,char2) - if column contain any value char1, char2 than return True.\n",
    "   1. to get isnot condition use ~ at start. **~df.column.isin(char1,char2)** -> **~** is like complementing the return value. this will return Fasle if condition is fullfiled.\n",
    "9.  **operator** - df.column!=30  -> ==,>,<,>,=>,=<,!=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To create dataframe manually**\n",
    "\n",
    "\n",
    ">from pyspark.sql.types import StructType,StructField,IntegerType,StringType\\\n",
    ">\n",
    ">--data to be inserted in dataframe\\\n",
    ">employee_data= [(1,\"a\",\"34\"),(2,\"b\",\"54\"),(3,\"c\",\"22\"),(4,\"d\",\"81\"),(5,\"e\",\"75\")]\\\n",
    ">\n",
    ">--to define schema\\\n",
    ">employee_scehma = StructType(StructField(\"id\",IntegerType(),False),StructField(\"name\",StringType(),True),StructField(\"number\",IntegerType(),False))\\\n",
    ">\\\n",
    ">**df=spark.createDataFrame(data=employee_data,scehma=employee_schema)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To add, drop, rename column in dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **To Add extra column to result including the data frame columns** - withColumn()\\\n",
    "    If the column is already present in dataframe thanwithColumn will update that column and only if column not present it will create another column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+----------+\n",
      "|collision_id|         crash_date|         crash_time|      on_street_name|number_of_persons_injured|number_of_persons_killed|contributing_factor_vehicle_1|contributing_factor_vehicle_2|  vehicle_type_code1|  vehicle_type_code2|borough|      date|\n",
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+----------+\n",
      "|     4456867|2021-09-02 00:00:00|2023-11-30 19:56:00|MAJOR DEEGAN EXPR...|                        0|                       0|                  Unspecified|                         null|               Sedan|                null|   null|2021-09-02|\n",
      "|     4456988|2021-09-11 00:00:00|2023-11-30 15:45:00|     BUSHWICK AVENUE|                        0|                       0|         Following Too Clo...|                  Unspecified|Station Wagon/Spo...|                null|   null|2021-09-11|\n",
      "|     4456859|2021-09-07 00:00:00|2023-11-30 22:30:00|          108 AVENUE|                        0|                       0|         Driver Inattentio...|                  Unspecified|Station Wagon/Spo...|Station Wagon/Spo...|   null|2021-09-07|\n",
      "|     4456663|2021-06-25 00:00:00|2023-11-30 14:35:00| BEACH CHANNEL DRIVE|                        0|                       0|                  Unspecified|                  Unspecified|       Pick-up Truck|                null| QUEENS|2021-06-25|\n",
      "|     4456624|2021-07-08 00:00:00|2023-11-30 18:40:00|AVENUE OF THE AME...|                        0|                       0|         Unsafe Lane Changing|                  Unspecified|               Sedan|               Sedan|   null|2021-07-08|\n",
      "+------------+-------------------+-------------------+--------------------+-------------------------+------------------------+-----------------------------+-----------------------------+--------------------+--------------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, date_format\n",
    "\n",
    "df=df.withColumn(\"date\",to_date(col(\"crash_date\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn(\"date\", to_date(col(\"crash_date\")))\\\n",
    "Adds another new column named \"date\" by applying the to_date function to convert the crash_date column to a date.\n",
    "\n",
    "using multiple withColumn statement we can add new columns.\n",
    "\n",
    "**df.select(\"col1\",\"col2\")**\\\n",
    "To get specific columns from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   **Add new column**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using constant literal use lit function** - lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+----------+\n",
      "|collision_id|         crash_date|date_column|new_column|\n",
      "+------------+-------------------+-----------+----------+\n",
      "|     4456867|2021-09-02 00:00:00| 2021/09/02|       ABD|\n",
      "|     4456988|2021-09-11 00:00:00| 2021/09/11|       ABD|\n",
      "|     4456859|2021-09-07 00:00:00| 2021/09/07|       ABD|\n",
      "|     4456663|2021-06-25 00:00:00| 2021/06/25|       ABD|\n",
      "|     4456624|2021-07-08 00:00:00| 2021/07/08|       ABD|\n",
      "+------------+-------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"new_column\", lit(\"ABD\")).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by calculation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we created another column which is based on calculating the remainder of the collision_id column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+---------+\n",
      "|collision_id|         crash_date|date_column|remainder|\n",
      "+------------+-------------------+-----------+---------+\n",
      "|     4456867|2021-09-02 00:00:00| 2021/09/02|       67|\n",
      "|     4456988|2021-09-11 00:00:00| 2021/09/11|       88|\n",
      "|     4456859|2021-09-07 00:00:00| 2021/09/07|       59|\n",
      "|     4456663|2021-06-25 00:00:00| 2021/06/25|       63|\n",
      "|     4456624|2021-07-08 00:00:00| 2021/07/08|       24|\n",
      "+------------+-------------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"remainder\",df.collision_id%100).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By concatinating two column values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+\n",
      "|collision_id|         crash_date|        id|\n",
      "+------------+-------------------+----------+\n",
      "|     4456867|2021-09-02 00:00:00|id 4456867|\n",
      "|     4456988|2021-09-11 00:00:00|id 4456988|\n",
      "|     4456859|2021-09-07 00:00:00|id 4456859|\n",
      "|     4456663|2021-06-25 00:00:00|id 4456663|\n",
      "|     4456624|2021-07-08 00:00:00|id 4456624|\n",
      "+------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat,col\n",
    "df = df.select(\"collision_id\",\"crash_date\").withColumn(\"id\", concat(lit(\"id \"),col(\"collision_id\").cast(\"string\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cast() -** To convert column datatype from one type to other type - col(col_name).cast(datatype)\n",
    "\n",
    "**col() -** In PySpark, the col function is used to reference a column in a DataFrame. It's a convenient way to refer to a column by name when performing operations or transformations on the data.\n",
    "\n",
    "**without col()** function we need to use dataframe reference to get the column - df.col_name \n",
    "\n",
    "**list()** - In PySpark, the lit function is used to create a new column with a constant literal value. It's short for \"literal.\" The lit function is often used when you want to add a new column to a DataFrame where all the values are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **To get the date in different format** - date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+\n",
      "|collision_id|         crash_date|date_column|\n",
      "+------------+-------------------+-----------+\n",
      "|     4456867|2021-09-02 00:00:00| 2021/09/02|\n",
      "|     4456988|2021-09-11 00:00:00| 2021/09/11|\n",
      "|     4456859|2021-09-07 00:00:00| 2021/09/07|\n",
      "|     4456663|2021-06-25 00:00:00| 2021/06/25|\n",
      "|     4456624|2021-07-08 00:00:00| 2021/07/08|\n",
      "+------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df = df.select(\"collision_id\",\"crash_date\").withColumn(\"date_column\", date_format(col(\"crash_date\"), \"yyyy/MM/dd\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  **To rename the column** - withColumnRenamed()\n",
    "\n",
    "This will create a new dataframe and in that name is changed not in the orignal dataset\n",
    "df.withColumnRenmae(\"old_col\",\"new_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+\n",
      "|collision_id|         crash_date|      date|\n",
      "+------------+-------------------+----------+\n",
      "|     4456867|2021-09-02 00:00:00|2021/09/02|\n",
      "|     4456988|2021-09-11 00:00:00|2021/09/11|\n",
      "|     4456859|2021-09-07 00:00:00|2021/09/07|\n",
      "|     4456663|2021-06-25 00:00:00|2021/06/25|\n",
      "|     4456624|2021-07-08 00:00:00|2021/07/08|\n",
      "+------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"date_column\",\"date\").show(5)\n",
    "\n",
    "# date_column name is changed to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  **To drop the column** - drop()\n",
    "\n",
    "To drop the column from the dataframe. \n",
    ">df.drop(\"col_name\")\n",
    "\n",
    "To drop multiple columns - using multiple drop fucntion we can drop multiple column e.g. drop().drop() or we can use packing unpacking method\n",
    ">drop_col =[\"col1\",\"col2\"]\\\n",
    ">df.drop(*drop_col)\n",
    "\n",
    "-- *drop_col- will unpack the list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[collision_id: int, crash_date: timestamp, date_column: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+\n",
      "|collision_id|         crash_date|date_column|\n",
      "+------------+-------------------+-----------+\n",
      "|     4456867|2021-09-02 00:00:00| 2021/09/02|\n",
      "|     4456988|2021-09-11 00:00:00| 2021/09/11|\n",
      "|     4456859|2021-09-07 00:00:00| 2021/09/07|\n",
      "|     4456663|2021-06-25 00:00:00| 2021/06/25|\n",
      "|     4456624|2021-07-08 00:00:00| 2021/07/08|\n",
      "+------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Joins in pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "syntax\n",
    "> &emsp;df1.join(df2,on_condition, hoe = \"joinning_type\")\n",
    "\n",
    "if not mentioned by default it is inner join\n",
    "\n",
    "> df1.join(df2, df1.id=df2.user_id)\n",
    "\n",
    "df1 - first datafarme, id - column of first dataframe\\\n",
    "df2 - second dataframe, user_id - column of seocnd dataframe\\\n",
    "    \n",
    "df1.id=df2.user_id - on condition on which two dataframe are connected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of join (joinning_type)**\n",
    "* **inner join** - return records which fullfill on conditons.\n",
    "* **full join** - left + right \n",
    "* **left outer join (left or left_outer)** - innerjoin + unmatched record according to conditoin in left datafarme\n",
    "   * All rows from the left DataFrame (df1) are included in the result. If there is no match in the right DataFrame (df2), the result will contain null values for the columns from df2.\n",
    "* **right outer join (right or right_outer)** -innerjoin + unmatched record according to conditoin in right datafarme\n",
    "  * All rows from the right DataFrame (df2) are included in the result. If there is no match in the left DataFrame (df1), the result will contain null values for the columns from df1\n",
    "* **left semi join (left_semi)** - Returns only the rows from the left DataFrame (df1) where there is at least one match in the right DataFrame (df2). Columns from the right DataFrame are not included in the resul\n",
    "  * similar to inner join but in result, column of only left datafarme are present.\n",
    "* **left anti join (left_anti)** - Returns only the rows from the left DataFrame (df1) where there is no match in the right DataFrame (df2). Columns from the right DataFrame are not included in the result.\n",
    "  * unmatched rows or rows which not follow the condition from left dataframe are in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
