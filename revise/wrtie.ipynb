{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.config(\"spark.driver.host\",'localhost').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x249a6ce3580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+-------+---+----------+-----+\n",
      "|   Name|Age|Occupation|Group|\n",
      "+-------+---+----------+-----+\n",
      "|  Alice| 25|  Engineer|    A|\n",
      "|    Bob| 30|   Analyst|    B|\n",
      "|Charlie| 22|    Intern|    C|\n",
      "|  David| 35|   Manager|    A|\n",
      "|    Eve| 28| Developer|    B|\n",
      "|  Frank| 32|  Designer|    C|\n",
      "|  Grace| 26| Scientist|    A|\n",
      "|  Harry| 40|   Manager|    B|\n",
      "|    Ivy| 31|   Analyst|    C|\n",
      "|   Jack| 29|  Engineer|    A|\n",
      "|   Kate| 27| Developer|    B|\n",
      "|    Leo| 33|  Designer|    C|\n",
      "|    Mia| 24| Scientist|    A|\n",
      "| Nathan| 38|  Engineer|    B|\n",
      "| Olivia| 23|   Analyst|    C|\n",
      "|  Peter| 36|   Manager|    A|\n",
      "|  Quinn| 34| Scientist|    B|\n",
      "| Rachel| 39| Developer|    C|\n",
      "|    Sam| 37|  Designer|    A|\n",
      "|   Tina| 21|    Intern|    B|\n",
      "+-------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# Sample data with 20 rows and 4 columns\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineer\", \"A\"),\n",
    "    (\"Bob\", 30, \"Analyst\", \"B\"),\n",
    "    (\"Charlie\", 22, \"Intern\", \"C\"),\n",
    "    (\"David\", 35, \"Manager\", \"A\"),\n",
    "    (\"Eve\", 28, \"Developer\", \"B\"),\n",
    "    (\"Frank\", 32, \"Designer\", \"C\"),\n",
    "    (\"Grace\", 26, \"Scientist\", \"A\"),\n",
    "    (\"Harry\", 40, \"Manager\", \"B\"),\n",
    "    (\"Ivy\", 31, \"Analyst\", \"C\"),\n",
    "    (\"Jack\", 29, \"Engineer\", \"A\"),\n",
    "    (\"Kate\", 27, \"Developer\", \"B\"),\n",
    "    (\"Leo\", 33, \"Designer\", \"C\"),\n",
    "    (\"Mia\", 24, \"Scientist\", \"A\"),\n",
    "    (\"Nathan\", 38, \"Engineer\", \"B\"),\n",
    "    (\"Olivia\", 23, \"Analyst\", \"C\"),\n",
    "    (\"Peter\", 36, \"Manager\", \"A\"),\n",
    "    (\"Quinn\", 34, \"Scientist\", \"B\"),\n",
    "    (\"Rachel\", 39, \"Developer\", \"C\"),\n",
    "    (\"Sam\", 37, \"Designer\", \"A\"),\n",
    "    (\"Tina\", 21, \"Intern\", \"B\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Occupation\", StringType(), True),\n",
    "    StructField(\"Group\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID     Name   Age\n",
      "0  1.0    Alice  25.0\n",
      "1  NaN      NaN   NaN\n",
      "2  3.0  Charlie  35.0\n",
      "3  4.0    David  40.0\n",
      "    ID     Name   Age   Salary Department\n",
      "0  1.0    Alice  25.0  50000.0         HR\n",
      "1  NaN      NaN   NaN      NaN        NaN\n",
      "2  3.0  Charlie  35.0  70000.0    Finance\n",
      "3  4.0    David  40.0  80000.0  Marketing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating two sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': [1, 6, 3, 4],\n",
    "    'Salary': [50000, 60000, 70000, 80000],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'Marketing']\n",
    "})\n",
    "\n",
    "# Performing a join on multiple columns using where() and filter()\n",
    "merged_df = df1.where(df1['ID'] == df2['ID']).filter(['ID', 'Name', 'Age']). \\\n",
    "    join(df2.where(df1['ID'] == df2['ID']).filter(['Salary', 'Department']))\n",
    "print(df1.where(df1['ID'] == df2['ID']).filter(['ID', 'Name', 'Age']))\n",
    "# s\n",
    "\n",
    "# Displaying the result\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-------+\n",
      "| id|   name|  id|subject|\n",
      "+---+-------+----+-------+\n",
      "|  1|  Alice|   1|   Math|\n",
      "|  2|    Bob|null|   null|\n",
      "|  3|Charlie|   3|English|\n",
      "+---+-------+----+-------+\n",
      "\n",
      "+---+-------+----+-------+\n",
      "| id|   name|  id|subject|\n",
      "+---+-------+----+-------+\n",
      "|  1|  Alice|   1|   Math|\n",
      "|  2|    Bob|null|   null|\n",
      "|  3|Charlie|   3|English|\n",
      "+---+-------+----+-------+\n",
      "\n",
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  3|Charlie|\n",
      "+---+-------+\n",
      "\n",
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  2| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(1, \"Math\"), (3, \"English\")], [\"id\", \"subject\"])\n",
    "\n",
    "# Perform a left outer join on the \"id\" column\n",
    "result = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left_outer\")\n",
    "result2 = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left\")\n",
    "result3 = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"leftsemi\")\n",
    "result4 = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"leftanti\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "result2.show()\n",
    "result3.show()\n",
    "result4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|           name|\n",
      "+---+---------------+\n",
      "|  1|[apple, orange]|\n",
      "|  2|[banana, grape]|\n",
      "|  3| [cherry, pear]|\n",
      "+---+---------------+\n",
      "\n",
      "+---+---------------+\n",
      "| id|           name|\n",
      "+---+---------------+\n",
      "|  1|[APPLE, ORANGE]|\n",
      "|  2|[BANANA, GRAPE]|\n",
      "|  3| [CHERRY, PEAR]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import transform\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.types import StringType,StructField,StructType,ArrayType\n",
    "\n",
    "# Sample DataFrame\n",
    "data1 = [(1, [\"apple\", \"orange\"]), (2, [\"banana\", \"grape\"]), (3, [\"cherry\", \"pear\"])]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with the corrected data\n",
    "df = spark.createDataFrame(data1, schema=schema)\n",
    "df.show()\n",
    "\n",
    "# Apply the transformation using withColumn\n",
    "transformed_df = df.withColumn(\"name\", transform(\"name\", lambda x: upper(x)).alias(\"name_upper\"))\n",
    "\n",
    "# Show the result\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+-------------+--------+\n",
      "| name|gender|gender_index|gender_onehot|features|\n",
      "+-----+------+------------+-------------+--------+\n",
      "| John|  Male|         0.0|(1,[0],[1.0])|   [1.0]|\n",
      "|Alice|Female|         1.0|    (1,[],[])|   [0.0]|\n",
      "|  Bob|  Male|         0.0|(1,[0],[1.0])|   [1.0]|\n",
      "+-----+------+------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"Male\"), (\"Alice\", \"Female\"), (\"Bob\", \"Male\")]\n",
    "schema = [\"name\", \"gender\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# List of transformations\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"gender_index\", outputCol=\"gender_onehot\")\n",
    "assembler = VectorAssembler(inputCols=[\"gender_onehot\"], outputCol=\"features\")\n",
    "\n",
    "# Fit and transform each transformation\n",
    "indexer_model = indexer.fit(df)\n",
    "df_indexed = indexer_model.transform(df)\n",
    "\n",
    "encoder_model = encoder.fit(df_indexed)\n",
    "df_encoded = encoder_model.transform(df_indexed)\n",
    "\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "# Show the result\n",
    "df_assembled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+\n",
      "| name|gender|full_name_gender|\n",
      "+-----+------+----------------+\n",
      "| John|  Male|     JOHN (Male)|\n",
      "|Alice|Female|  ALICE (Female)|\n",
      "|  Bob|  Male|      BOB (Male)|\n",
      "+-----+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,upper\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"Male\"), (\"Alice\", \"Female\"), (\"Bob\", \"Male\")]\n",
    "schema = [\"name\", \"gender\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# User-defined function\n",
    "@udf(StringType())\n",
    "def concatenate_name_gender(name, gender):\n",
    "    return f\"{name.upper()} ({gender})\"\n",
    "\n",
    "# Apply the UDF\n",
    "df_result = df.withColumn(\"full_name_gender\", concatenate_name_gender(df[\"name\"], df[\"gender\"]))\n",
    "\n",
    "# Show the result\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x194205b5580>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|gender|\n",
      "+-----+------+\n",
      "| John|  Male|\n",
      "|Alice|Female|\n",
      "|  Bob|  Male|\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+------------+\n",
      "|Name |gender|Created Name|\n",
      "+-----+------+------------+\n",
      "|John |Male  |JOHN        |\n",
      "|Alice|Female|ALICE       |\n",
      "|Bob  |Male  |BOB         |\n",
      "+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,upper\n",
    "\n",
    "def up(x):\n",
    "    return x.upper()\n",
    "upperCaseUDF = udf(up, StringType())\n",
    "# change is the name with which we can access this fucntion, this is the registered name for upperCase function.\n",
    "\n",
    "data = [(\"John\", \"Male\"), (\"Alice\", \"Female\"), (\"Bob\", \"Male\")]\n",
    "schema = [\"Name\", \"gender\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Custom UDF with withColumn()\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"Created Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|gender|\n",
      "+-----+------+\n",
      "| John|  Male|\n",
      "|Alice|Female|\n",
      "|  Bob|  Male|\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+--------------+\n",
      "|Name |gender|Created Name  |\n",
      "+-----+------+--------------+\n",
      "|John |Male  |[JOHN, john]  |\n",
      "|Alice|Female|[ALICE, alice]|\n",
      "|Bob  |Male  |[BOB, bob]    |\n",
      "+-----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,upper,lower\n",
    "\n",
    "def up(x):\n",
    "    return [x.upper(),x.lower()]\n",
    "upperCaseUDF = udf(up,ArrayType(StringType()))\n",
    "# change is the name with which we can access this fucntion, this is the registered name for upperCase function.\n",
    "\n",
    "data = [(\"John\", \"Male\"), (\"Alice\", \"Female\"), (\"Bob\", \"Male\")]\n",
    "schema = [\"Name\", \"gender\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Custom UDF with withColumn()\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"Created Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|gender|\n",
      "+-----+------+\n",
      "| John|  Male|\n",
      "|Alice|Female|\n",
      "|  Bob|  Male|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\itsrk\\AppData\\Local\\Temp\\ipykernel_15160\\2047332330.py\", line 4, in <lambda>\n  File \"C:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 160, in wrapped\n    return f(*args, **kwargs)\nTypeError: upper() missing 1 required positional argument: 'col'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Custom UDF with withColumn()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCreated Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupperCaseUDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    905\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    906\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m         },\n\u001b[0;32m    910\u001b[0m     )\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\itsrk\\AppData\\Local\\Temp\\ipykernel_15160\\2047332330.py\", line 4, in <lambda>\n  File \"C:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 160, in wrapped\n    return f(*args, **kwargs)\nTypeError: upper() missing 1 required positional argument: 'col'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,upper\n",
    "\n",
    "\n",
    "upperCaseUDF = udf(lambda x:upper(), StringType())\n",
    "# change is the name with which we can access this fucntion, this is the registered name for upperCase function.\n",
    "\n",
    "data = [(\"John\", \"Male\"), (\"Alice\", \"Female\"), (\"Bob\", \"Male\")]\n",
    "schema = [\"Name\", \"gender\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Custom UDF with withColumn()\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"Created Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       Name|Age|\n",
      "+-----------+---+\n",
      "|   John,Doe| 25|\n",
      "|Alice,Smith| 30|\n",
      "|Bob,Johnson| 22|\n",
      "+-----------+---+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Split the \"Name\" column by commas and apply flatMap\u001b[39;00m\n\u001b[0;32m     14\u001b[0m df_flatmap \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames\u001b[39m\u001b[38;5;124m\"\u001b[39m, split(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mrdd \\\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m row: [(row\u001b[38;5;241m.\u001b[39mAge, name\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m row\u001b[38;5;241m.\u001b[39mNames])\n\u001b[1;32m---> 18\u001b[0m \u001b[43mdf_flatmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df_flatmap\u001b[38;5;241m.\u001b[39mcollect():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"John,Doe\", 25), (\"Alice,Smith\", 30), (\"Bob,Johnson\", 22)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Split the \"Name\" column by commas and apply flatMap\n",
    "df_flatmap = df.withColumn(\"Names\", split(col(\"Name\"), \",\")) \\\n",
    "    .select(\"Age\", \"Names\") \\\n",
    "    .rdd \\\n",
    "    .flatMap(lambda row: [(row.Age, name.strip()) for name in row.Names])\n",
    "df_flatmap.toDF.show()\n",
    "# Display the result\n",
    "for row in df_flatmap.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map DataFrame:\n",
      "+--------------------------------+\n",
      "|words                           |\n",
      "+--------------------------------+\n",
      "|[Hello, World]                  |\n",
      "|[PySpark, Map, FlatMap, Example]|\n",
      "|[OpenAI, GPT-3]                 |\n",
      "+--------------------------------+\n",
      "\n",
      "\n",
      "FlatMap DataFrame:\n",
      "+-------+\n",
      "|word   |\n",
      "+-------+\n",
      "|Hello  |\n",
      "|World  |\n",
      "|PySpark|\n",
      "|Map    |\n",
      "|FlatMap|\n",
      "|Example|\n",
      "|OpenAI |\n",
      "|GPT-3  |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# Example data\n",
    "data = [\"Hello World\", \"PySpark Map FlatMap Example\", \"OpenAI GPT-3\"]\n",
    "\n",
    "# Create an RDD from the data\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Example using map transformation\n",
    "# Transform each element by splitting the words\n",
    "map_result = rdd.map(lambda sentence: sentence.split(\" \"))\n",
    "\n",
    "# Convert map_result into a DataFrame\n",
    "map_df = map_result.map(lambda x: Row(words=x)).toDF()\n",
    "\n",
    "# Show the map DataFrame\n",
    "print(\"Map DataFrame:\")\n",
    "map_df.show(truncate=False)\n",
    "\n",
    "# Example using flatMap transformation\n",
    "# Transform each element by splitting the words and flatten the results\n",
    "flatMap_result = rdd.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "\n",
    "# Convert flatMap_result into a DataFrame\n",
    "flatMap_df = flatMap_result.map(lambda x: Row(word=x)).toDF()\n",
    "\n",
    "# Show the flatMap DataFrame\n",
    "print(\"\\nFlatMap DataFrame:\")\n",
    "flatMap_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a function to print each row\n",
    "def print_row(row):\n",
    "    print(\"Processing row:\", row)\n",
    "\n",
    "# Apply foreach to print each row\n",
    "df.rdd.foreach(print_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+\n",
      "| name|Female|Male|\n",
      "+-----+------+----+\n",
      "|  Bob|    20|  50|\n",
      "| John|    30|  10|\n",
      "|Alice|    20|null|\n",
      "+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"Male\", 10),\n",
    "        (\"Alice\", \"Female\", 20),\n",
    "        (\"Bob\", \"Male\", 10),\n",
    "        (\"John\", \"Female\", 30),\n",
    "        (\"Bob\", \"Female\", 20),\n",
    "        (\"Bob\", \"Male\", 40)]\n",
    "\n",
    "columns = [\"name\", \"gender\", \"age_group\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "pivot_df = df.groupBy(\"name\").pivot(\"gender\").sum(\"age_group\")\n",
    "\n",
    "# Show the result\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----+-------+-------+\n",
      "|Player_Name|     Stadium|Runs|Wickets|Catches|\n",
      "+-----------+------------+----+-------+-------+\n",
      "|     Jadeja|    Wankhede|  40|      2|      1|\n",
      "|     Hardik|Eden Gardens|  55|      0|      2|\n",
      "|     Jadeja|Eden Gardens|  25|      3|      0|\n",
      "|     Watson|    Wankhede|  30|      1|      0|\n",
      "|     Hardik|    Wankhede|  45|      0|      1|\n",
      "|     Jadeja|    Wankhede|  60|      1|      1|\n",
      "|     Hardik|Eden Gardens|  45|      0|      3|\n",
      "|     Hardik|Eden Gardens|  30|      4|      0|\n",
      "|     Watson|    Wankhede|  20|      2|      2|\n",
      "|     Watson|Eden Gardens|  50|      0|      0|\n",
      "+-----------+------------+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_data = [\n",
    "    (\"Jadeja\", 'Wankhede', 40, 2, 1),\n",
    "    (\"Hardik\", 'Eden Gardens', 55, 0, 2),\n",
    "    (\"Jadeja\", 'Eden Gardens', 25, 3, 0),\n",
    "    (\"Watson\", 'Wankhede', 30, 1, 0),\n",
    "    (\"Hardik\", 'Wankhede', 45, 0, 1),\n",
    "    (\"Jadeja\", 'Wankhede', 60, 1, 1),\n",
    "    (\"Hardik\", 'Eden Gardens', 45, 0, 3),\n",
    "    (\"Hardik\", 'Eden Gardens', 30, 4, 0),\n",
    "    (\"Watson\", 'Wankhede', 20, 2, 2),\n",
    "    (\"Watson\", 'Eden Gardens', 50, 0, 0)\n",
    "]\n",
    "\n",
    "\n",
    "match_schema = \"Player_Name String , Stadium String , Runs int , Wickets int , Catches int\"\n",
    "match_df = spark.createDataFrame(data = match_data, schema = match_schema)\n",
    "\n",
    "match_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+-------------------------+------------------+---------------------+\n",
      "|Player_Name|Eden Gardens_sum(Runs)|Eden Gardens_sum(Wickets)|Wankhede_sum(Runs)|Wankhede_sum(Wickets)|\n",
      "+-----------+----------------------+-------------------------+------------------+---------------------+\n",
      "|     Jadeja|                    25|                        3|               100|                    3|\n",
      "|     Watson|                    50|                        0|                50|                    3|\n",
      "|     Hardik|                   130|                        4|                45|                    0|\n",
      "+-----------+----------------------+-------------------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df=match_df.groupBy('Player_Name').pivot('Stadium').sum(\"Runs\",\"Wickets\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+------------------------+\n",
      "|Player_Name|Eden Garden_sum(Runs)|Eden Garden_sum(Wickets)|\n",
      "+-----------+---------------------+------------------------+\n",
      "|     Jadeja|                 null|                    null|\n",
      "|     Watson|                 null|                    null|\n",
      "|     Hardik|                 null|                    null|\n",
      "+-----------+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df=match_df.groupBy('Player_Name').pivot('Stadium',['Eden Garden']).sum(\"Runs\",\"Wickets\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+----+----+----+\n",
      "|Player_Name|  20|  25|  30|  40|  45|  50|  55|  60|\n",
      "+-----------+----+----+----+----+----+----+----+----+\n",
      "|     Jadeja|null|   3|null|   2|null|null|null|   1|\n",
      "|     Watson|   2|null|   1|null|null|   0|null|null|\n",
      "|     Hardik|null|null|   4|null|   0|null|   0|null|\n",
      "+-----------+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df=match_df.groupBy('Player_Name').pivot('Runs').sum(\"Wickets\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+------+\n",
      "|         Player|Match1|Match2|Match3|\n",
      "+---------------+------+------+------+\n",
      "|    Virat Kohli|    85|   100|    75|\n",
      "|    Steve Smith|    90|   105|    80|\n",
      "|Kane Williamson|    88|    95|    70|\n",
      "+---------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cricket_data = [(\"Virat Kohli\", 85, 100, 75),\n",
    "        (\"Steve Smith\", 90, 105, 80),\n",
    "        (\"Kane Williamson\", 88, 95, 70)]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(cricket_data, [\"Player\", \"Match1\", \"Match2\", \"Match3\"])\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_expr=\"stack(3,'Match11',Match1, 'Match22',Match2, 'Match33',Match3) as (Match,Score) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----+\n",
      "|         Player|  Match|Score|\n",
      "+---------------+-------+-----+\n",
      "|    Virat Kohli|Match11|   85|\n",
      "|    Virat Kohli|Match22|  100|\n",
      "|    Virat Kohli|Match33|   75|\n",
      "|    Steve Smith|Match11|   90|\n",
      "|    Steve Smith|Match22|  105|\n",
      "|    Steve Smith|Match33|   80|\n",
      "|Kane Williamson|Match11|   88|\n",
      "|Kane Williamson|Match22|   95|\n",
      "|Kane Williamson|Match33|   70|\n",
      "+---------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_df=df.select(\"Player\",expr(stack_expr))\n",
    "unpivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-----+\n",
      "| id|position|key|value|\n",
      "+---+--------+---+-----+\n",
      "|  1|       0|  A|   10|\n",
      "|  1|       1|  B|   20|\n",
      "|  2|       0|  A|   15|\n",
      "|  2|       1|  C|   25|\n",
      "|  2|       2|  D|   30|\n",
      "|  3|       0|  D|   35|\n",
      "|  3|       1|  B|   22|\n",
      "+---+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "\n",
    "# Example DataFrame\n",
    "data = [(1, {\"A\": 10, \"B\": 20}),\n",
    "        (2, {\"A\": 15, \"C\": 25, \"D\": 30}),\n",
    "        (3, {\"B\": 22, \"D\": 35})]\n",
    "\n",
    "columns = [\"id\", \"dict_column\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Explode the dictionary column using posexplode\n",
    "df_exploded = df.select(\"id\", posexplode(\"dict_column\").alias(\"position\", \"key\", \"value\"))\n",
    "\n",
    "# Show the result\n",
    "df_exploded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|     input|formatted_date|\n",
      "+----------+--------------+\n",
      "|2020-02-01|    01/02/2020|\n",
      "|2019-03-01|    01/03/2019|\n",
      "|2021-03-01|    01/03/2021|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"),date_format(col(\"input\"),\"dd/MM/yyyy\").alias(\"formatted_date\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
