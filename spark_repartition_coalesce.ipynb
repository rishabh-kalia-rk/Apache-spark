{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks spark: Repartitoin vs coalesce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition playes important role in performance improvement, error handling  and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need of partition strategy\n",
    "* Adopting best partition strategy is designing best performance in spark application.\n",
    "* It is important to choose right number of partitions and size of each parition.\n",
    "  * e.g.\n",
    "    * we did 10 paritions and there are 16 cores so suring processing each core will process each parititon and as there are only 10 parition (parittion cannot be further breakdown so to spilt it among the cores) then these paritions will be processed by 10 core nut in this the remaining 6 core will remain ideal or unused which is wastage of resources. Thus we try to parition the data into number of cores present.\n",
    "* Evenly distributed parition improves the performace, unevenly distributed hits the performance.\n",
    "  * with even distribution reources are effectively used like all cores are used. \n",
    "  * try to make parition equal to cores or number should be multiple of number of cores.\n",
    "* Size of the partition should be choosen carefully.\n",
    "  * lets say only one partition is created with size of 500 MB in a worker node with 16 core. One partition can't be shared among cores. so one core would be processing 500MB data where 15 cores kept idle. thus \n",
    "  * or lets say one parittion is of size 1GB and other 100MB thus partition of 100MB will be processed quickly and for 1GB it will take time. thus the core which processes 100MB file will remain in idle state for large amount of time which is like wastage of the services thus we should do partition in proper size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default paritions for RDD and dataframe**\n",
    "* The parameter **sc.defaultParallelism** determines the number of partition when creating data within spark. default value is 8 so it create 8 parition by default.\n",
    "  * sc.defaultParallelsim - when creating data within spark.\n",
    "* when reading data from external system, paritions are created based on parameter **spark.sql.files.maxPartitionBytes** which is by default 128 MB.\n",
    "  * sc.files.maxPartitionByte - when we are reading file from external source. The spark will paritition the file basesd on size of 128 MB each partition.\n",
    "  * Applicable only when file is splitable, it should not be in zip format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Repartition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function reparition is used to increase and decrease the partition in spark.\n",
    "* Repartition always shuffle the data and build new partition from the scratch.\n",
    "  * shufflying is costlier process.\n",
    "  * full shuffle is done in this before repartition.\n",
    "* repartition result in almost equal sized partition. After the shufflying repartition is done in which parittion of resultant file is done in equal size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
