{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks spark Broadcast variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a programming mechanism in pyspark through which we can keep only read only copy of data into each node of the cluster instaed of sending it to node every time a task is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.\n",
    "\n",
    "(for example CA to California, NY to New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code).\n",
    "\n",
    "**lookup table** - A lookup table in PySpark is a way to **search for a value or a range of values** in a table or a database using PySpark.\n",
    "\n",
    "Instead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache this lookup info on each machine and tasks use this cached info while executing the transformations. info like CA = California, NY = New York\n",
    "\n",
    "**mainly used in joining operation**\n",
    "*  If one table or dataframe have many record and other has less record. Than it is not idead to send the fact table or big table to all the worker node because it will take hugh memory and also execution time increase. So the tiny table are sent to the worker node as whole and the fact table is distributed among the worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary -\n",
    "* Broadcase variable is materialised and cached in each worker node.\n",
    "* It avoid data shuffle thus improve performace. As copy is present one each node thus if worker node need that data then there is no need of fetching data from other node or shufflying of data as entire dataset is already present on the worker node.\n",
    "* it reduces network I/O operation thus improves performance.\n",
    "* Ideal for joinning sistuation where one side of join is tiny table.\n",
    "* Tiny table is sent to all worker node through driver thus the size of tiny table must be less than driver.\n",
    "* only suitable tables as it gets cached in each node. It created for large tables, it would consume memory of each worker node thus hitting the performance.\n",
    "* Broadcast variable is sent to worker nodes through driver. So the size of broadcast variable should fit into driver memory otherwise leading to OOM issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local system\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.config(\"spark.driver.host\",\"localhost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Sample Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "data = [(1, 'Alice', 25),\n",
    "        (2, 'Bob', 30),\n",
    "        (3, 'Charlie', 22),\n",
    "        (4, 'David', 28),\n",
    "        (5, 'Eva', 35)]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = [\"store_id\", \"name\", \"amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create sample dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = [(1, \"store_london\"),\n",
    "        (2, 'store_paris'), \n",
    "        (3, 'store_frankfurt'),\n",
    "        (4, 'store_stockholm'),\n",
    "        (5,'store_oslo') \n",
    "]\n",
    "\n",
    "storeDf=spark.createDataFrame(data=store,schema=['store_id','store_name'])\n",
    "storeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinDF=df.join(broadcast(storeDf),storeDf.store_id==df.store_id)\n",
    "joinDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *  using PySpark to perform a join operation on two DataFrames, where storeDf is broadcasted, and you are joining on the store_id column. This is a common pattern to optimize join operations, especially when one of the DataFrames is relatively small and can be efficiently broadcasted to all the nodes in the cluster.\n",
    "  *  The use of broadcast hints to Spark that storeDf is small enough to be efficiently broadcasted to all nodes, reducing the need for shuffling data across the network during the join operation.\n",
    "  *   it's beneficial to use broadcasting when one DataFrame is significantly smaller than the other. If both DataFrames are large, broadcasting may not provide significant performance improvements, and Spark's default behavior for joins will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Broadcast Variable Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. creating the dataframe and storing the array of rown in the variable.\n",
    "    DF.collect() returns Array of Row type of the dataframe\n",
    "    * first we create the dataframe\n",
    "    * use the collectin function to get the dataframe value in form of rows\n",
    "    * than it is stored in the broadcast variable.\n",
    "    * broadcast_df\n",
    "      * it contain the reference of the broadcast variable.\n",
    "        * e.g. <pyspark.broadcast.Broadcast object at 0x000000>\n",
    "      * using value, we can get the value of the broadcast variable.\n",
    "      * for row in broadcast_value:\n",
    "        * to retrive all the value from broadcast_value variable.\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "When you use broadcast(), you typically use it in the context of the driver program to broadcast data to all the worker nodes. one common approach is to collect the data you want to broadcast on the driver node using df.collect() (or other means), and then use sparkContext.broadcast() to broadcast it to the worker nodes.\n",
    "\n",
    "* Here's a summary of the typical workflow:\n",
    "\n",
    "* Collect Data on the Driver: You collect the data you want to broadcast on the driver node. This typically involves collecting the data from a DataFrame or any other source into a suitable data structure.\n",
    "\n",
    "* Broadcast Data: Once you have the data on the driver node, you use sparkContext.broadcast() to broadcast it. This broadcasts the data to all the worker nodes in the Spark cluster.\n",
    "\n",
    "* Accessing Broadcasted Data: On the worker nodes, you can access the broadcasted data using the .value attribute of the broadcast variable. This allows the tasks running on the worker nodes to access the broadcasted data efficiently.\n",
    "\n",
    "* Performing Operations: With the broadcasted data available on all worker nodes, you can perform distributed operations using Spark's transformations and actions.\n",
    "\n",
    "This approach allows you to efficiently distribute read-only data to all the worker nodes in the Spark cluster without incurring significant overhead from network transfer.\n",
    "  \n",
    "  \n",
    "Error -\n",
    "  * **RuntimeError:** It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063\n",
    "  * if we directly use df (spark.sparkContext.broadcast(df.collect())) in place of df.collect()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('first').getOrCreate()\n",
    "\n",
    "# Create a schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a 2D DataFrame\n",
    "data = [(1, 10), (2, 20), (3, 30)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\"\"\"\n",
    "+---+-----+\n",
    "| id|value|\n",
    "+---+-----+\n",
    "|  1|   10|\n",
    "|  2|   20|\n",
    "|  3|   30|\n",
    "+---+-----+\n",
    "\"\"\"\n",
    "\n",
    "# Add the DataFrame to a broadcast variable\n",
    "broadcast_df = spark.sparkContext.broadcast(df.collect())\n",
    "\n",
    "# Retrieve the value from the broadcast variable\n",
    "broadcast_value = broadcast_df.value\n",
    "print(broadcast_value)\n",
    "# Print the values from the broadcast variable\n",
    "for row in broadcast_value:\n",
    "    print(row)\n",
    "\"\"\"\n",
    "[Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30)]\n",
    "Row(id=1, value=10)\n",
    "Row(id=2, value=20)\n",
    "Row(id=3, value=30)\n",
    "\"\"\"\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Storing a constant value in the broadcast variable.\n",
    "    * broadcast_age_threshold = spark.sparkContext.broadcast(30)\n",
    "    * now the broadcast_age_threshold contain the reference of broadcast where value 30 is stored.\n",
    "    * using broadcast_age_threshold.value will get the value of the varibale stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.host','localhost').appName(\"Broadcast\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"John\", 30), (\"Jane\", 25), (\"Alice\", 35)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# Broadcast a Python object\n",
    "broadcast_age_threshold = spark.sparkContext.broadcast(30)\n",
    "\n",
    "# Use the broadcast variable in a transformation\n",
    "filtered_df = df.filter(df.age > broadcast_age_threshold.value)\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  In PySpark, there's a built-in broadcast() function available in the pyspark.sql.functions module. This function allows you to explicitly mark a DataFrame to be broadcasted during join operations. When you use broadcast(df) with a DataFrame df, it indicates to Spark that you want to broadcast df when performing join operations with other DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"shop\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = spark.createDataFrame([(1, 'Alice'), (2, 'Bob'), (3, 'Charlie')], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(1, 'Engineer'), (2, 'Manager'), (3, 'Analyst')], [\"id\", \"role\"])\n",
    "\n",
    "# Broadcast df1 during join operation\n",
    "joined_df = df1.join(broadcast(df2), on='id', how='left')\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n",
    "\n",
    "# Don't forget to stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We have two DataFrames df1 and df2.\n",
    "* We use broadcast(df2) to explicitly mark df2 for broadcasting during the join operation with df1.\n",
    "* The join operation is performed between df1 and the broadcasted df2.\n",
    "* By broadcasting df2, Spark optimizes the join operation by distributing df2 to all worker nodes, reducing the data shuffle during the join.\n",
    "\n",
    "\n",
    "Using broadcast() from pyspark.sql.functions is convenient when you want to explicitly control the broadcasting behavior for join operations without manually managing broadcast variables. However, it's important to note that broadcast() is specifically designed for broadcast joins and may not be suitable for general broadcasting of large read-only data across all tasks in the cluster. For that purpose, you would still use sparkContext.broadcast()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **To check the differene in performance with and without use of broadcast variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time without broadcast: 0.12572813034057617\n",
      "Elapsed time with broadcast: 0.06737208366394043\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"performace\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample large DataFrame\n",
    "large_df = spark.range(1000000)\n",
    "\n",
    "# Sample small DataFrame\n",
    "small_df = spark.createDataFrame([(1, 'Alice'), (2, 'Bob'), (3, 'Charlie')], [\"id\", \"name\"])\n",
    "\n",
    "# Without broadcasting small DataFrame\n",
    "start_time = time.time()\n",
    "result1 = large_df.join(small_df, on='id', how='left')\n",
    "elapsed_time_without_broadcast = time.time() - start_time\n",
    "\n",
    "# With broadcasting small DataFrame\n",
    "start_time = time.time()\n",
    "result2 = large_df.join(broadcast(small_df), on='id', how='left')\n",
    "elapsed_time_with_broadcast = time.time() - start_time\n",
    "\n",
    "# Print the elapsed time for both scenarios\n",
    "print(\"Elapsed time without broadcast:\", elapsed_time_without_broadcast)\n",
    "print(\"Elapsed time with broadcast:\", elapsed_time_with_broadcast)\n",
    "\n",
    "# Don't forget to stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
