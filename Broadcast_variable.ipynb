{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks spark Broadcast variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a programming mechanism in psark through which we can keep only read only copy of data into each node of the cluster instaed of sending it to node every time a task is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.\n",
    "\n",
    "(for example CA to California, NY to New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code).\n",
    "\n",
    "**lookup table** - A lookup table in PySpark is a way to **search for a value or a range of values** in a table or a database using PySpark.\n",
    "\n",
    "Instead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache this lookup info on each machine and tasks use this cached info while executing the transformations. info like CA = California, NY = New York\n",
    "\n",
    "**mainly used in joining operation**\n",
    "*  If one table or dataframe have many record and other has less record. Than it is not idead to send the fact table or big table to all the worker node because it will take hugh memory and also execution time increase. So the tiny table are sent to the worker node as whole and the fact table is distributed among the worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary -\n",
    "* Broadcase variable is materialised and cached in each worker node.\n",
    "* It avoid data shuffle thus improve performace. As copy is present one each node thus if worker node need that data then there is no need of fetching data from other node or shufflying of data as entire dataset is already present on the worker node.\n",
    "* it reduces network I/O operation thus improves performance.\n",
    "* Ideal for joinning sistuation where one side of join is tiny table.\n",
    "* Tiny table is sent to all worker node through driver thus the size of tiny table must be less than driver.\n",
    "* only suitable tables as it gets cached in each node. It created for large tables, it would consume memory of each worker node thus hitting the performance.\n",
    "* Broadcast variable is sent to worker nodes through driver. So the size of broadcast variable should fir into driver memory otherwise leading to OOM issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local system\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.config(\"spark.driver.host\",\"localhost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Sample Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "data = [(1, 'Alice', 25),\n",
    "        (2, 'Bob', 30),\n",
    "        (3, 'Charlie', 22),\n",
    "        (4, 'David', 28),\n",
    "        (5, 'Eva', 35)]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = [\"store_id\", \"name\", \"amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create sample dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = [(1, \"store_london\"),\n",
    "        (2, 'store_paris'), \n",
    "        (3, 'store_frankfurt'),\n",
    "        (4, 'store_stockholm'),\n",
    "        (5,'store_oslo') \n",
    "]\n",
    "\n",
    "storeDf=spark.createDataFrame(data=store,schema=['store_id','store_name'])\n",
    "storeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinDF=df.join(broadcast(storeDf),storeDf.store_id==df.store_id)\n",
    "joinDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *  using PySpark to perform a join operation on two DataFrames, where storeDf is broadcasted, and you are joining on the store_id column. This is a common pattern to optimize join operations, especially when one of the DataFrames is relatively small and can be efficiently broadcasted to all the nodes in the cluster.\n",
    "  *  The use of broadcast hints to Spark that storeDf is small enough to be efficiently broadcasted to all nodes, reducing the need for shuffling data across the network during the join operation.\n",
    "  *   it's beneficial to use broadcasting when one DataFrame is significantly smaller than the other. If both DataFrames are large, broadcasting may not provide significant performance improvements, and Spark's default behavior for joins will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
