{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **To connect Azure datalake to databricks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mehtods of Azure data lakes** \n",
    "\n",
    "> ADLS integration with data bricks\n",
    "> \n",
    "> - Using a service principal\n",
    "> - using Azure active directory credentials knows as a credential passthrough\n",
    "> - Using ADLS access key directly\n",
    "> - Creating mount found using ADLS access key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### **Follow and execute these code in your Databricks platform.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1: Using ADLS access key directly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we have two configure properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set (\n",
    "\"fs.azure.account.key.<storage account>.dfs.core.window.net\",\n",
    "\"<access key>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   \\<storage account\\> - \n",
    "    *   for this you need to create a storage account\n",
    "        *   Provide a unique name for your Storage account. The name must be unique across Azure.\n",
    "*   \\<access keys\\>\n",
    "    *   Go to storage account and on left you can access keys in security + networking section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To list out the files in the contianer**\n",
    "*   Container is like folder in azure data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://<container_name>@<storage_account_name>.dfs.core.window.net/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To read the file present in the container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data lake file location\n",
    "file_location = \"abfss://<container_name>@<storage_account_name>.dfs.core.window.net/\"\n",
    "\n",
    "# under this folder we have csv file which we will convert to dataframe.\n",
    "\n",
    "# to create data frame from file\n",
    "# the file is fetched/extracted from azur data storage into databric and we create dataframe.\n",
    "df= spark.read.format(\"csv\").option(\"inferschema\",True).option(\"header\",True).option(\"delimiter\",\",\").load(file_location)\n",
    "\n",
    "# to display the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2: Creating Mount Point using ADLS Access Key**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once mount point is created than databricks can consider Azur data lake storage as local file system even if its not local file system.\n",
    "\n",
    "It will read and write as if it is local file system for databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this we need to set some parameters**\n",
    "*   source\n",
    "    *   Go to storage account, click on  go to resources.\n",
    "    *   On left in setting find endpoint setting and click on that.\n",
    "    *   Copy the Primary endpoint Blob services. \n",
    "    *   copy the part after https://\n",
    "    *   source = \"wasbs://<container_name>@<copied_part>\"\n",
    "    *   wasbs - part of syntax to connect using blob services.\n",
    "*   mount_point\n",
    "    *    It's essentially the directory path on the Spark cluster where you want to access the files from the mounted storage.\n",
    "    *    The mount_point is set to \"/mnt/\". This means that the content of the storage container specified in the source parameter will be mounted at the \"/mnt/\" directory on the Spark cluster.\n",
    "    *   For example, if you set mount_point=\"/my_mount_point/\", the content of the storage container will be accessible under the \"/my_mount_point/\" directory on the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\n",
    "source = \"wasbs://<container_name>@<storage_account_name>.blob.core.windows.net\",\n",
    "mount_point = \"/mnt/adls_test\"\n",
    "extra_configs={\"fs.azure.account.key.<storage_account_name>.blob.core.windows.net\":\"<acess key>\"}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now Mount point is created successfully**\n",
    "    *   Now we don't have to mention the whole path of azure data storage to fect the data we can get that from mount point folder also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list out the files present in the path mentioned.\n",
    "dbutils.fs.ls(\"/mnt/adls_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To list out the mount point we created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To unmount the point**\n",
    "*   dbutils.fs.unmount(\"point\")\n",
    "*   point - ths path or point which you want to mount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/adls_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
