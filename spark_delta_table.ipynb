{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Delta tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.host\", \"localhost\").getOrCreate()\n",
    "\n",
    "#to build session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('rdddf').master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note - Run commands on databricks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **creating datalake table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.create(spark)\\\n",
    ".tableName(\"delta_internal_demo\")\\\n",
    ".addColumn(\"emp_id\",\"INT\")\\\n",
    ".addColumn(\"emp_name\",\"STRING\")\\\n",
    ".addColumn(\"makrs\",\"INT\")\\\n",
    ".property(\"description\",\"table craeted for demo purpose\")\\\n",
    ".location(\"dbfs:/FileStore/delta\")\\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On execution**\n",
    "* In data - DAatbase Tables a table is created with name delta_internal_demo\n",
    "* In DBFS - at dbfs:/FileStore/delta\" fiel is created with name _delta_log\n",
    "* If we insert some data to table than that data file will be also stored in same location.\n",
    "* Every operation that we execute on the delta table will be recorded in the _delta_log folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/FileStore/delta/_delta_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This will list all the files present at the location mentioned.**\n",
    "* within this there will be json file to store the logs\n",
    "* crc file Cyclic redundant check\n",
    "* some other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "head dbfs:/delta/_delta_log/00000000000000000000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the json log file.\n",
    "* as we have craeted delta table so this file craete info about delta table creation operation.\n",
    "* Miainly info about metadata of table created. column anme, type etc.\n",
    "* who created, when created, about user info also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta_internal_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert data into table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into delta_internal_demo values(100,\"ab\",70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on execution of command on delta table a new json log file is created in sequence.like 00000000000000000001.json means its 2nd log file created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note - data file will also created in delta folder. In the form of parquet file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To read the content of the log file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "head dbfs:/delta/_delta_log/00000000000000000001.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is based on current insertion operation of data into delta table.\n",
    "* {\"add\":{\"path\":\"part-00000-4dc3c8a4-6a1a-46ee-97ba-c75f7c443ff4-c000.snappy.parquet\"\n",
    "  * add - it shows add operation is dome\n",
    "  * path - data file name. The insertion of data created a data file.\n",
    "  * contain ither info aslo like size, modification time, number of record affected, min value, max value of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into delta_internal_demo values(200,\"philip\",65);\n",
    "insert into delta_internal_demo values(300,\"Lara\",68);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Each insertion operation is considered seprate operation and thus for each a new log file is created.\n",
    "*  after 10 log file a parquet file is created which will contian info about the 10 saved log files.\n",
    "   *  d00000000000000000010.checkpoint.parquet - this is file created after 10 log files.\n",
    "*  For each insertion a datafile is created.\n",
    "*  In log file of deletion operation\n",
    "   *  Operation - remove\n",
    "   *  path - data file name tht should not be considered during select query or for user output.When engine will read this file than it will get understand not to read these data files.\n",
    "*  '    \n",
    "\n",
    "**soft delete pattern**\n",
    "* Which means even we are not intersted in particualr data file it will not be removed immidiately. there is deafault time period for this of 7 days after that it will be deleted. But we can configure this parameter and than we can delete it immidiately. \n",
    "*  After we apply delete operation\n",
    "   *  when we apply selelct query than engine will go throgh log files and there have the info to not to consider paritcular files from the data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To read data present in checkpoint file**\n",
    "* As parquet file is not human readable hence we have to create dataframe of that file to get and understand the info.\n",
    "* To read the info present in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format(\"parquet\").load(\"dbfs:/delta/_delta_log/00000000000000000010.checkpoint.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The file contain the info of previous 10 operation done on the delta table. so with change in operation type on delta table the content of the checkpoint file will also change.\n",
    "* it contain many column like add, remove, metadata, etc and engine using this info can get to know which daat file to include and which not to. thus info of all the 10 log file get in one single file and because of that the engine don't have to travese through each and every 10 log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How different operation works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **update** - E.g. update statement is given to delta lake table.\n",
    "  * e.g. id=100 there will be 3 data files. Engine will scan all the data fiels or in the storage location and fetch the fiels where we are having suitable record to be updated.\n",
    "  * out of all files lets say only 3 files identified for update than only these 3 fiels will be loaded to memory or ram.\n",
    "  * Than in memeory the engine will scan the entire data in the files and make the required update.\n",
    "  * Than it will wrtie the data back to storage system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **create delta table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute commands in databricks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - Pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.create(spark)\\\n",
    "    .tableName(\"shop\")\\\n",
    "        .addColumn(\"shop_id\",\"INT\")\\\n",
    "            .addColumn(\"name\",\"STRING\")\\\n",
    "                .addColumn(\"sale\",\"INT\")\\\n",
    "                    .property(\"description\",'table create for demo purpose')\\\n",
    "                        .location(\"shop/\").execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* location\n",
    "  * the location where this table should be created and if no value is given than it will create the table in hive meta store.\n",
    "* description\n",
    "  * In this we can mention the pupose of creating table or short note about table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the table is already created or present than this command will give error**\n",
    "* for this we can use craeteIfNotExist command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.createIfNotExists(spark)\\\n",
    "    .tableName(\"shop\")\\\n",
    "        .addColumn(\"shop_id\",\"INT\")\\\n",
    "            .addColumn(\"name\",\"STRING\")\\\n",
    "                .addColumn(\"sale\",\"INT\")\\\n",
    "                    .property(\"description\",'table create for demo purpose')\\\n",
    "                        .location(\"shop/\").execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if the tabel is already creaetd but we want to replace the table or table is not created**\n",
    "* we can use createOrReplace\n",
    "* If the table is not craeted than it will create.\n",
    "* if table is present than it will drop the previous one and create the new one or create the new structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.createOrReplace(spark)\\\n",
    "    .tableName(\"shop\")\\\n",
    "        .addColumn(\"shop_id\",\"INT\")\\\n",
    "            .addColumn(\"name\",\"STRING\")\\\n",
    "                .addColumn(\"sale\",\"INT\")\\\n",
    "                    .property(\"description\",'table create for demo purpose')\\\n",
    "                        .location(\"shop/\").execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In table name**\n",
    "    * we can mention the database name also db.tab_name\n",
    "    * default database is  - default\n",
    "    * But we can mention the database name if want to store the table in other database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To check the new created table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from shop;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Method 2 - Create delta table using SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE shop (\n",
    "shop_id INT,\n",
    "name STRING,\n",
    "sale INT,\n",
    ") USING DELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If table is already present than this can give error so use IF NOT EXIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXIST shop (\n",
    "shop_id INT,\n",
    "name STRING,\n",
    "sale INT,\n",
    ") USING DELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If table is already present in the system it will drop the previous version and recreate using the new syntax or structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE shop\n",
    "(\n",
    "shop_id INT,\n",
    "name STRING,\n",
    "sale INT,\n",
    ") USING DELTA\n",
    "LOCATION '/path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **path** \n",
    "    * location where we want to store the table\n",
    "* **location**\n",
    "  * it can be dbfs or s3 bucket etc.So this table will be created on top of thta external storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Method 3 -using existing dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_data= [(100,\"virat\",34), (200,\"kohli\",97),(100,\"hunter\",80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_schema= [\"shop_id\",\"name\",\"sale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.createDataFrame(data=shop_data,schema=shop_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|shop_id|  name|sale|\n",
      "+-------+------+----+\n",
      "|    100| virat|  34|\n",
      "|    200| kohli|  97|\n",
      "|    100|hunter|  80|\n",
      "+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create table in metastore using dataframe schema and write data to it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.fromat(\"delta\").saveTable(\"shop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **format(\"delta\")**\n",
    "  * engine will understand that we need to create delta table\n",
    "* saveTable(\"db.shop\")\n",
    "  * db - it is the name of the database in which you want to stor the table\n",
    "  * shop - name of the table\n",
    "  * In this we are creating the table but not defining the structure of the table\n",
    "    * the structure, datatypes of the table will be taken from the stucture of datafarme from which the table is being created.\n",
    "* In the previous methods we have created only structure of table and not inserted any data.\n",
    "  * But when we create delta table using dataframe than the data present in the dataframe will be inserted into delta table own its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Delta table isntance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delta table instance**\n",
    "* To make soft link to actual delta table\n",
    "* It will not create copy of the actual delta table.\n",
    "* If any operation or changes done in actual table will be reflected in instance table.\n",
    "* If any operation done on instance table than that will be reflected on orignal table.\n",
    "* any operation we do with delta table or on its instance than it will be reflected in both.\n",
    "\n",
    "Note -\n",
    "  * Delta table instance is the replica of master table or actual table but it still pointing to same memory there is no copy created when we create instance.\n",
    "  * If we only want to use pyspark or scala programming than we go for delta table instance creation.\n",
    "\n",
    "**Usage of it**\n",
    "* for applying dml query on delta table we are dependent on SQL query language.\n",
    "* In some cases if we don't want to use sql query and want to use pyspark in that case we can use table instance.As we cannot use table name to exectue the operaion like we do in dataframe.\n",
    "  \n",
    "  **To create table instance***\n",
    "  * using path - path -> where we have created the delta table.\n",
    "    * > **DeltaTable.forPath(saprk,\"/path/to/table\")**\n",
    "  * using table name\n",
    "    * > **DeltaTable.forName(spark,\"table_name\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First creating delta table using taht we will create delta table insatnce**\n",
    "In real life project we generally create delta table uding the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.createIfNotExists(spark)\\\n",
    "    .tableName(\"shop\")\\\n",
    "        .addColumn(\"shop_id\",\"INT\")\\\n",
    "            .addColumn(\"name\",\"STRING\")\\\n",
    "                .addColumn(\"sale\",\"INT\")\\\n",
    "                    .property(\"description\",'table create for demo purpose')\\\n",
    "                        .location(\"/FileStore/shop/delta\").execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**location(\"/FileStore/shop/delta\")**\n",
    "* Location where delta table will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To insert some data to the delta table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into delta_internal_demo values(100,\"philip\",65);\n",
    "insert into delta_internal_demo values(200,\"Lara\",68);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM shop;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To create delta table instance** \n",
    "suppose we don't want to use sql queries and use pysaprk operations in that we need to create delta table instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method1 - using forPath**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltainstance1 = DeltaTable.forPath(spark,\"/FileStore/shop/delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/FileStore/shop/delta**\n",
    "* location where delta table is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaDF= deltainstance1.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To see the dataframe created**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deltaDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sql vs pyspark commnad example-**\n",
    "delete record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "delete from shop where shop_id=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deltainstance1.delete(\"shop_id=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2 - using forName**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltainstance2 = DeltaTable.forName(spark,\"shop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **shop**\n",
    "    * shop is the name of the delta table\n",
    "* **deltainstance2**\n",
    "  * It is the delta table instance created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create dataframe of delta instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaDF= deltainstance2.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sql vs pyspark operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "DESCRIBE HISTORY shop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This sql command will give the history of operation done on the shop table.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deltainstance2.hoistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pysaprk command to get the detail about history of operatoins on delta table instance or delta table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Insert data into delta table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - SQL Style insert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO shop values ((100,\"philip\",65),(200,\"Lara\",68));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To display the data from table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from shop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2 - Dataframe insert** - insert data to delta table using dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,StructType,IntegerType "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To define the structure of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_schema= StructType([\\\n",
    "    StructField(\"shop_id\",IntegerType,True),\\\n",
    "    StructField(\"name\",StringType,True),\\\n",
    "    StructField(\"sale\",IntegerType,True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_data= [(100,\"virat\",34), (200,\"kohli\",97),(100,\"hunter\",80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data=shop_data,schema=shop_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To insert the dataframe into delta table using write command** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"shop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **mode**\n",
    "  * append - if we want to retain prvious data than use append\n",
    "  * overwrite - will delete the previous data store in table and than add the current data.\n",
    "* **saveAsTable**\n",
    "  * this is the table to which we want to store the data.\n",
    "* **format** \n",
    "  * the format of the table to which we want to store the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To insert the dataframe into delta table using insertInto command** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"shop\",overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**insertInto(\"shop\",overwrite=False)**\n",
    "* shop - name of the delta tbale to which we want to insert the data.\n",
    "* overwrite - \n",
    "  * false - to retain the previous data.\n",
    "  * true - to overwrite the current data on previous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 3 - using Temp view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"delta_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create temporary view using the dataframe df and name of the view is called delta_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command is to query the temporary view created from df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "insert into shop\n",
    "select * from delta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **using above command we insert data into the shop delta table**\n",
    "* delta_data\n",
    "  * It is the temporary view created from df dataframe.\n",
    "* shop \n",
    "  * it is the delta table to which we want to insert the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 4 - Spark SQL insert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"insert into shop select * from delta_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Delta table delete**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "delete from shop where shop_id=100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2 - SQL suing delta location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "delete from delta.'/delta/table/path/' where shop_id=100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/delta/table/path/** - location where delta table is present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 3 saprk sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"delete from shop where shop_id=100;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sql() - within this we can write the sql statement and we don't need to us the %sql or magic word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 4 - using delta table instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark,\"shop\")\n",
    "\n",
    "#deltaTable is the instance.\n",
    "\n",
    "deltaTable.delete(\"shop_id=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **update delta table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - using SQL standad table name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "update shop set name = \"kartik\" where shop_id =3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - using SQL standad table path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "update delta.'/delta/table/path/' set name = \"kartik\" where shop_id =3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **/delta/table/path/**\n",
    "  * This is the location where delta table is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 4 - Pyspark standard using Table instance** - can use forPath or using froName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark,\"/delta/table/path/\")\n",
    "\n",
    "#declare the predicate by using a sql formatted string.\n",
    "\n",
    "deltaTable.update(condition=\"shop_id=100\", set={\"salary\":\"15000\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **deltaTable = DeltaTable.forPath(spark,\"/delta/table/path/\")**\n",
    "   *  **spark** - SparkSession object\n",
    "   *  **/delta/table/path/** - locatoin where delta table is present\n",
    "   * **DeltaTable** - library,In PySpark, the DeltaTable class is part of the Delta Lake library, which extends Apache Spark to enable ACID transactions, schema evolution, and other features for data lakes. The DeltaTable API provides functionalities to interact with Delta tables, such as reading, writing, updating, and deleting data.\n",
    "   * **deltaTable** - it is the delta table instance.\n",
    "\n",
    "*  deltaTable.update()\n",
    "   *  To update the instance\n",
    "   *  condition=\"shop_id=100\" - this is like filter condition to get the required rows which we want to update.\n",
    "   *  set={\"salary\":\"15000\"} - this is the set condition to set the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **merge(upsert) using pyspark and sql**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* upsert - update or insert if any match occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_schema= StructType([\\\n",
    "    StructField(\"shop_id\",IntegerType,True),\\\n",
    "    StructField(\"name\",StringType,True),\\\n",
    "    StructField(\"sale\",IntegerType,True)])\n",
    "\n",
    "shop_data= [(100,\"virat\",34), (200,\"kohli\",97),(100,\"hunter\",80)]\n",
    "# this is the source data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data=shop_data,schema=shop_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create delta lake table of name dim_employee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "CREATE OR REPLACE TABLE dim_shop (\n",
    "    shop_id INT,\n",
    "    name STRING,\n",
    "    sale INT\n",
    ") USING DELTA \n",
    "LOCATION \"/delta/table/path/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "select * from dim_shop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **As there is no record in the dim_shop thus it will fetch nothing.**\n",
    "* dim_shop - is the target table. according to condition we will do insert or update into this table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"source_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df is the dataframe.\n",
    "* from df we craete a temporary view called source_view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "select * from source_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "select * from dim_shop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement to execute merge operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "\n",
    "MERGE INTO dim_shop as target\n",
    "USING source_view as source\n",
    "on target.shop_id=source.emp_id\n",
    "\n",
    "WHEN MATCHED\n",
    "THEN UPDATE SET\n",
    "\n",
    "target.shop_id=source.shop_id\n",
    "target.name=source.name\n",
    "target.sales=source.sale\n",
    "\n",
    "WHEN NOT MATCHED THEN \n",
    "INSERT (shop_id,name,sale) VALUES (shop_id,name,sale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **target.shop_id=source.emp_id**\n",
    "   *  when this condition is true than update the values with the new value from source\n",
    "*  If conditoin fails than insert the record\n",
    "*  taget- target is the delta table or dim_shop table\n",
    "*  source - it is the dataframe from which we created the view called source_view from where we will insert the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2 -  using pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create dataframe. This will be the source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_schema= StructType([\\\n",
    "    StructField(\"shop_id\",IntegerType,True),\\\n",
    "    StructField(\"name\",StringType,True),\\\n",
    "    StructField(\"sale\",IntegerType,True)])\n",
    "\n",
    "shop_data= [(100,\"virat\",34), (200,\"kohli\",97),(100,\"hunter\",80)]\n",
    "# this is the source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data=shop_data,schema=shop_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create the delta table. This will be our target table or **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "delta_df=DeltaTable.forPath(spark,\"/delta/table/path/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **/delta/table/path/**\n",
    "  * This path contain the delta table\n",
    "  * dim-shop - is the delta table name or target delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pysaprk condition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df.alias(\"target\").merge(\n",
    "source=df.alias(\"source\"),\n",
    "condition = \"target.shop_id=source.shop_id\").whenMatchedUpdate(set=\n",
    "    {\n",
    "    \"shop_id\":\"source.shop_id\",\n",
    "    \"name\":\"source.name\",\n",
    "    \"sale\":\"source.sale\"}\n",
    "    ).whenNotMatchedInsert(\n",
    "    values = {\n",
    "\n",
    "        \"shop_id\":\"source.shop_id\", \n",
    "        \"name\":\"source.name\",\n",
    "        \"sale\":\"source.sale\",\n",
    "   }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "select * from dim_shop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
